{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio classification model training and evaluation (Red Dragon AI)",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZsPaKNroZBG",
        "outputId": "86c093f5-9ed6-4c6c-c1b8-d960061f1b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "%%time\n",
        "!cp \"/content/drive/My Drive/reddragon_spectrograms.zip\" \"reddragon_spectrograms.zip\"\n",
        "!unzip \"reddragon_spectrograms.zip\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n",
            "Archive:  reddragon_spectrograms.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of reddragon_spectrograms.zip or\n",
            "        reddragon_spectrograms.zip.zip, and cannot find reddragon_spectrograms.zip.ZIP, period.\n",
            "CPU times: user 71.8 ms, sys: 20.9 ms, total: 92.7 ms\n",
            "Wall time: 39.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q8FzKr0bWPx"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfmoo3dFwrMJ"
      },
      "source": [
        "#CNN model training and evaluation\n",
        "\n",
        "Using spectrograms to classify speakers. \n",
        "\n",
        "40 speakers: 95% val accuracy. \n",
        "\n",
        "80 speakers: 85% val accuracy\n",
        "\n",
        "115 speakers: 78% val accuracy. \n",
        "\n",
        "Using triplet loss with the penultimate latent vector revealed that the model was unable to extrapolate to unseen speakers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1iOAVQQv8zJ"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjMbp4UrDjL"
      },
      "source": [
        "clip_info = pd.read_csv(\"/content/drive/My Drive/clip_info_finalest.csv\")\n",
        "\n",
        "classes = clip_info.speaker_id.unique()\n",
        "classes = classes[0:112]\n",
        "num_classes = len(classes)\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-e_hLpZb4jQ"
      },
      "source": [
        "%%time\n",
        "'''\n",
        "clip_info['count'] = 0\n",
        "from PIL import Image\n",
        "from collections import defaultdict \n",
        "\n",
        "counter = defaultdict(lambda:0)\n",
        "\n",
        "root = \"/content/reddragon/\"\n",
        "for i in range(len(clip_info)):\n",
        "  row = clip_info.iloc[i]\n",
        "  speaker = str(row[\"speaker_id\"])\n",
        "  \n",
        "  counter[speaker] += 1\n",
        "  clip_info.loc[i, \"count\"] = counter[speaker]\n",
        " \n",
        "'''\n",
        "'''\n",
        "%%time\n",
        "from PIL import Image\n",
        "from collections import defaultdict \n",
        "\n",
        "counter = defaultdict(lambda:0)\n",
        "\n",
        "root = \"/content/reddragon/\"\n",
        "for i in range(len(clip_info)):\n",
        "  row = clip_info.iloc[i]\n",
        "  speaker = str(row[\"speaker_id\"])\n",
        "  count = str(row[\"count\"])\n",
        "\n",
        "  clip_info.loc[i, \"final_path\"] = \"/content/content/drive/My Drive/reddragon/spectrograms/\" + speaker + \"/\" + count + \".npy\"\n",
        "'''\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNtxP1Upb587"
      },
      "source": [
        "\n",
        "root = \"/content/content/drive/My Drive/reddragon/spectrograms/\"\n",
        "train_df = pd.DataFrame(columns=clip_info.columns)\n",
        "test_df = pd.DataFrame(columns=clip_info.columns)\n",
        "test_pct = 0.1\n",
        "for classi in classes:\n",
        "  \n",
        "  class_exs = clip_info[clip_info['speaker_id'] == int(classi)].reset_index(drop=True)\n",
        "  num_exs = class_exs.shape[0]\n",
        "  train_num = int((1-test_pct) * num_exs)\n",
        "  \n",
        "  train_exs = class_exs.iloc[:train_num]\n",
        "  test_exs = class_exs.iloc[train_num:]\n",
        "  if test_exs.shape[0] == 0:\n",
        "    print(num_exs)\n",
        "    print(train_num)\n",
        "    print(classi)\n",
        "\n",
        "  train_df = pd.concat([train_df, train_exs])\n",
        "  test_df = pd.concat([test_df, test_exs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eUrWO2knACS"
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "def build_cnn_model(num_classes):\n",
        "\n",
        "  inp = Input((128, 378, 1))\n",
        "  x = Conv2D(filters=16, kernel_size=2, activation='relu')(inp)\n",
        "  x = MaxPooling2D(pool_size=2)(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  x = Conv2D(filters=32, kernel_size=2, activation='relu')(x)\n",
        "  x = MaxPooling2D(pool_size=2)(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "\n",
        "  x = Conv2D(filters=64, kernel_size=2, activation='relu')(x)\n",
        "  x = MaxPooling2D(pool_size=2)(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  x = Conv2D(filters=128, kernel_size=2, activation='relu')(x)\n",
        "  x = MaxPooling2D(pool_size=2)(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  out = Dense(num_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(inputs=inp, outputs=out)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eQ1fWSKc35J"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def get_spec(filename, data_mean=-26.533236022216165, spec_width=378):\n",
        "  fn = bytes.decode(filename.numpy())\n",
        "  spec = np.load(fn)\n",
        "  spec /= data_mean\n",
        "\n",
        "  num_times = spec_width // spec.shape[1]\n",
        "  \n",
        "  if num_times != 0:\n",
        "    leftover = spec_width - num_times * spec.shape[1]\n",
        "\n",
        "    spec = np.repeat(spec, num_times, axis=1)\n",
        "    \n",
        "    spec = np.concatenate([spec, spec[:,:leftover]], axis=1)\n",
        "  else:\n",
        "    spec = spec[:,:spec_width]\n",
        "\n",
        "  \n",
        "  spec = np.expand_dims(spec, axis=2)\n",
        "  \n",
        "\n",
        "  return spec\n",
        "\n",
        "def _parse_function(filename, label):\n",
        "  \n",
        "  spec = tf.py_function(get_spec, [filename], tf.float64)\n",
        "  spec = tf.ensure_shape(spec ,shape=(128, 378, 1))\n",
        "  label = tf.ensure_shape(label ,shape=(num_classes))\n",
        "  \n",
        "  \n",
        "  return spec, label\n",
        "\n",
        "batch_size=16\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(train_df['final_path'].values, tf.string),\n",
        "                                                    tf.cast(train_labels.values, tf.int32) ))\n",
        "train_dataset = train_dataset.map(_parse_function)\n",
        "train_dataset = train_dataset.shuffle(5000)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(test_df['final_path'].values, tf.string),\n",
        "                                                    tf.cast(test_labels.values, tf.int32) ))\n",
        "valid_dataset = valid_dataset.map(_parse_function)\n",
        "valid_dataset = valid_dataset.shuffle(5000)\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "valid_dataset = valid_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjgjcMYSZSYr",
        "outputId": "a93c2508-6be9-4e4e-eba2-db6effef5f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        }
      },
      "source": [
        "model = build_cnn_model(num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 128, 378, 1)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 127, 377, 16)      80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 63, 188, 16)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 63, 188, 16)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 62, 187, 32)       2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 31, 93, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 31, 93, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 30, 92, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 15, 46, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 15, 46, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 45, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 22, 128)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 7, 22, 128)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 112)               14448     \n",
            "=================================================================\n",
            "Total params: 57,760\n",
            "Trainable params: 57,760\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D3UEzU8YR3o",
        "outputId": "79af1e5f-4f14-4b76-da42-ed673d215ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "K.set_value(model.optimizer.learning_rate, 0.0002)\n",
        "history = model.fit( train_dataset, \n",
        "                    steps_per_epoch = train_df.shape[0]//16,\n",
        "                   epochs = 8,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = test_df.shape[0]//16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "703/703 [==============================] - 148s 210ms/step - loss: 0.5266 - accuracy: 0.8442 - val_loss: 0.9771 - val_accuracy: 0.7199\n",
            "Epoch 2/8\n",
            "703/703 [==============================] - 147s 209ms/step - loss: 0.5226 - accuracy: 0.8436 - val_loss: 0.9974 - val_accuracy: 0.7199\n",
            "Epoch 3/8\n",
            "703/703 [==============================] - 148s 210ms/step - loss: 0.5029 - accuracy: 0.8548 - val_loss: 0.7973 - val_accuracy: 0.7809\n",
            "Epoch 4/8\n",
            "703/703 [==============================] - 147s 209ms/step - loss: 0.5016 - accuracy: 0.8509 - val_loss: 0.8528 - val_accuracy: 0.7600\n",
            "Epoch 5/8\n",
            "703/703 [==============================] - 147s 209ms/step - loss: 0.4719 - accuracy: 0.8620 - val_loss: 0.7883 - val_accuracy: 0.7647\n",
            "Epoch 6/8\n",
            "703/703 [==============================] - 147s 209ms/step - loss: 0.4688 - accuracy: 0.8585 - val_loss: 0.7767 - val_accuracy: 0.7739\n",
            "Epoch 7/8\n",
            "703/703 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.8654"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iro2QWn-nfsw"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/115voices_vanillacnn.h5\")\n",
        "from tensorflow import keras\n",
        "model = keras.models.load_model(\"/content/drive/My Drive/115voices_vanillacnn.h5\")\n",
        "kout = Dense(150, activation='softmax')(model.layers[-2].output)\n",
        "\n",
        "non_class1 = clip_info[clip_info.speaker_id ==int(non_classes[0])].reset_index(drop=True)\n",
        "non_class2 = clip_info[clip_info.speaker_id ==int(non_classes[1])].reset_index(drop=True)\n",
        "non_class3 = clip_info[clip_info.speaker_id ==int(non_classes[2])].reset_index(drop=True)\n",
        "\n",
        "n1 = get_spec_norm(non_class1.loc[0, 'final_path'])\n",
        "n2 = get_spec_norm(non_class1.loc[1, 'final_path'])\n",
        "n3 = get_spec_norm(non_class1.loc[2, 'final_path'])\n",
        "\n",
        "o1 = get_spec_norm(non_class3.loc[0, 'final_path'])\n",
        "o2 = get_spec_norm(non_class3.loc[1, 'final_path'])\n",
        "o3 = get_spec_norm(non_class3.loc[2, 'final_path'])\n",
        "\n",
        "\n",
        "m1 = get_spec_norm(non_class2.loc[0, 'final_path'])\n",
        "m2 = get_spec_norm(non_class2.loc[1, 'final_path'])\n",
        "m3 = get_spec_norm(non_class2.loc[2, 'final_path'])\n",
        "\n",
        "np.linalg.norm(dummy_model(np.expand_dims(o2, axis=0))-dummy_model(np.expand_dims(n3, axis=0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvkYDpqJescL"
      },
      "source": [
        "#Tabular model training and evaluation\n",
        "\n",
        "Using tabular features to classify speakers. \n",
        "\n",
        "150 speakers: 99.5% val accuracy. \n",
        "\n",
        "Using triplet loss with the penultimate layer revealed that the latent vector could extrapolate to a holdout set of 101 speakers with an accuracy of 70% without training. (Fine-tuning would probably allow it to go back to 99% val accuracy)\n",
        "\n",
        "This suggests that a useful, but imperfect, general representation has been learned.\n",
        "\n",
        "Training is extremely quick with this model, you can run it yourself, given the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAeIdvSahiBF"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqRAs2Y3hJS-",
        "outputId": "210d9ae9-8657-4f13-a59e-58adaff5a09c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "clip_info = pd.read_csv(\"/content/drive/My Drive/clip_info_finalest.csv\")\n",
        "\n",
        "classes = clip_info.speaker_id.unique()\n",
        "classes = classes[0:150]\n",
        "num_classes = len(classes)\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLxFcfSb4hfH"
      },
      "source": [
        "def build_tabular(num_classes):\n",
        "\n",
        "  inp = Input((187))\n",
        "  x = Dense(193, activation='relu')(inp)\n",
        "  x = Dropout(0.1)(x)\n",
        "\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  out = Dense(num_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(inputs=inp, outputs=out)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HGBeXc7mq1v"
      },
      "source": [
        "X = np.load(\"/content/drive/My Drive/tab_info_complete.npy\")\n",
        "y = clip_info.speaker_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j2h-1Fn-9Cl"
      },
      "source": [
        "X = pd.DataFrame(X)\n",
        "total = pd.concat([X,y], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1niBAuYKA2Ti",
        "outputId": "2cb90f64-8663-408c-b88c-47785c06dacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "total"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>speaker_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-378.984647</td>\n",
              "      <td>116.162422</td>\n",
              "      <td>-17.332546</td>\n",
              "      <td>35.479327</td>\n",
              "      <td>-4.448054</td>\n",
              "      <td>-8.969773</td>\n",
              "      <td>-10.835938</td>\n",
              "      <td>-9.472341</td>\n",
              "      <td>-0.795535</td>\n",
              "      <td>-4.003963</td>\n",
              "      <td>-3.411418</td>\n",
              "      <td>-6.772533</td>\n",
              "      <td>-3.136346</td>\n",
              "      <td>-0.438471</td>\n",
              "      <td>-0.724784</td>\n",
              "      <td>-7.967132</td>\n",
              "      <td>-3.204955</td>\n",
              "      <td>-2.498293</td>\n",
              "      <td>-4.416882</td>\n",
              "      <td>-0.013168</td>\n",
              "      <td>-4.519273</td>\n",
              "      <td>-1.344529</td>\n",
              "      <td>-0.017705</td>\n",
              "      <td>-3.099055</td>\n",
              "      <td>-0.163226</td>\n",
              "      <td>3.991476</td>\n",
              "      <td>0.624113</td>\n",
              "      <td>1.014821</td>\n",
              "      <td>5.007116</td>\n",
              "      <td>0.611839</td>\n",
              "      <td>2.227606</td>\n",
              "      <td>3.690647</td>\n",
              "      <td>1.047732</td>\n",
              "      <td>2.435556</td>\n",
              "      <td>3.660017</td>\n",
              "      <td>3.157895</td>\n",
              "      <td>4.023052</td>\n",
              "      <td>5.619310</td>\n",
              "      <td>3.962789</td>\n",
              "      <td>3.374655</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005883</td>\n",
              "      <td>0.004814</td>\n",
              "      <td>0.004310</td>\n",
              "      <td>0.004318</td>\n",
              "      <td>0.004430</td>\n",
              "      <td>0.002984</td>\n",
              "      <td>0.003827</td>\n",
              "      <td>0.006852</td>\n",
              "      <td>0.004911</td>\n",
              "      <td>0.002691</td>\n",
              "      <td>0.001568</td>\n",
              "      <td>0.002570</td>\n",
              "      <td>0.003436</td>\n",
              "      <td>0.001212</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>4.885913e-07</td>\n",
              "      <td>2.752604e-08</td>\n",
              "      <td>3.310267e-10</td>\n",
              "      <td>2.096735e-10</td>\n",
              "      <td>1.028704e-10</td>\n",
              "      <td>1.084731e-10</td>\n",
              "      <td>2.539558e-10</td>\n",
              "      <td>1.981922e-10</td>\n",
              "      <td>8.486392e-10</td>\n",
              "      <td>3.387747e-10</td>\n",
              "      <td>8.125983e-10</td>\n",
              "      <td>2.940645e-10</td>\n",
              "      <td>1.915570e-10</td>\n",
              "      <td>16.781012</td>\n",
              "      <td>19.563837</td>\n",
              "      <td>20.597696</td>\n",
              "      <td>19.110058</td>\n",
              "      <td>18.907353</td>\n",
              "      <td>21.324619</td>\n",
              "      <td>40.286322</td>\n",
              "      <td>6818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-360.585734</td>\n",
              "      <td>109.734169</td>\n",
              "      <td>-28.806914</td>\n",
              "      <td>35.320974</td>\n",
              "      <td>-11.222565</td>\n",
              "      <td>-1.842546</td>\n",
              "      <td>-14.212534</td>\n",
              "      <td>-10.677975</td>\n",
              "      <td>1.110187</td>\n",
              "      <td>-0.334412</td>\n",
              "      <td>-3.172706</td>\n",
              "      <td>-12.516410</td>\n",
              "      <td>-1.608148</td>\n",
              "      <td>-3.096722</td>\n",
              "      <td>-1.703256</td>\n",
              "      <td>-8.087596</td>\n",
              "      <td>-3.407032</td>\n",
              "      <td>-0.335436</td>\n",
              "      <td>-6.325538</td>\n",
              "      <td>-1.191407</td>\n",
              "      <td>-2.294444</td>\n",
              "      <td>-1.416659</td>\n",
              "      <td>0.618330</td>\n",
              "      <td>1.412627</td>\n",
              "      <td>1.166945</td>\n",
              "      <td>5.897295</td>\n",
              "      <td>2.338395</td>\n",
              "      <td>1.359338</td>\n",
              "      <td>6.999274</td>\n",
              "      <td>3.117167</td>\n",
              "      <td>4.644802</td>\n",
              "      <td>3.254507</td>\n",
              "      <td>3.017500</td>\n",
              "      <td>5.550849</td>\n",
              "      <td>4.500174</td>\n",
              "      <td>5.781449</td>\n",
              "      <td>3.431529</td>\n",
              "      <td>1.439598</td>\n",
              "      <td>2.080126</td>\n",
              "      <td>-0.521632</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004059</td>\n",
              "      <td>0.003550</td>\n",
              "      <td>0.005577</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>0.005960</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.004485</td>\n",
              "      <td>0.004437</td>\n",
              "      <td>0.003569</td>\n",
              "      <td>0.003476</td>\n",
              "      <td>0.004212</td>\n",
              "      <td>0.003283</td>\n",
              "      <td>0.002065</td>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.001269</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>5.305461e-07</td>\n",
              "      <td>2.561801e-08</td>\n",
              "      <td>2.972462e-10</td>\n",
              "      <td>3.035355e-10</td>\n",
              "      <td>2.145632e-10</td>\n",
              "      <td>2.034855e-10</td>\n",
              "      <td>5.124118e-10</td>\n",
              "      <td>3.488708e-10</td>\n",
              "      <td>1.206213e-09</td>\n",
              "      <td>4.410794e-10</td>\n",
              "      <td>1.147120e-09</td>\n",
              "      <td>3.965186e-10</td>\n",
              "      <td>4.237231e-10</td>\n",
              "      <td>15.783335</td>\n",
              "      <td>20.178407</td>\n",
              "      <td>20.849809</td>\n",
              "      <td>19.250531</td>\n",
              "      <td>18.917857</td>\n",
              "      <td>20.290513</td>\n",
              "      <td>41.420199</td>\n",
              "      <td>6818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-385.111883</td>\n",
              "      <td>103.916242</td>\n",
              "      <td>-9.390188</td>\n",
              "      <td>31.750364</td>\n",
              "      <td>-6.380552</td>\n",
              "      <td>-5.126309</td>\n",
              "      <td>-8.498947</td>\n",
              "      <td>-9.582734</td>\n",
              "      <td>-2.340875</td>\n",
              "      <td>-5.162678</td>\n",
              "      <td>-2.530776</td>\n",
              "      <td>-6.240352</td>\n",
              "      <td>-4.714329</td>\n",
              "      <td>-1.416646</td>\n",
              "      <td>-0.342624</td>\n",
              "      <td>-6.329395</td>\n",
              "      <td>-1.965251</td>\n",
              "      <td>-3.140808</td>\n",
              "      <td>-4.357819</td>\n",
              "      <td>2.612710</td>\n",
              "      <td>-3.939102</td>\n",
              "      <td>-5.190391</td>\n",
              "      <td>-0.843375</td>\n",
              "      <td>-2.071385</td>\n",
              "      <td>-4.466790</td>\n",
              "      <td>-0.285233</td>\n",
              "      <td>-0.554399</td>\n",
              "      <td>-1.545056</td>\n",
              "      <td>2.557443</td>\n",
              "      <td>1.655296</td>\n",
              "      <td>2.597475</td>\n",
              "      <td>2.883823</td>\n",
              "      <td>3.817268</td>\n",
              "      <td>4.195601</td>\n",
              "      <td>4.166037</td>\n",
              "      <td>6.105434</td>\n",
              "      <td>4.458746</td>\n",
              "      <td>4.813243</td>\n",
              "      <td>5.400831</td>\n",
              "      <td>3.343224</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003416</td>\n",
              "      <td>0.002760</td>\n",
              "      <td>0.002376</td>\n",
              "      <td>0.003159</td>\n",
              "      <td>0.004918</td>\n",
              "      <td>0.004074</td>\n",
              "      <td>0.003360</td>\n",
              "      <td>0.002468</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.002150</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.001187</td>\n",
              "      <td>0.002023</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>2.255360e-07</td>\n",
              "      <td>1.041009e-08</td>\n",
              "      <td>3.469450e-10</td>\n",
              "      <td>3.366587e-10</td>\n",
              "      <td>2.549752e-10</td>\n",
              "      <td>2.579850e-10</td>\n",
              "      <td>4.479804e-10</td>\n",
              "      <td>3.764807e-10</td>\n",
              "      <td>7.868146e-10</td>\n",
              "      <td>4.642241e-10</td>\n",
              "      <td>7.525490e-10</td>\n",
              "      <td>3.804914e-10</td>\n",
              "      <td>3.316418e-10</td>\n",
              "      <td>17.257269</td>\n",
              "      <td>18.635043</td>\n",
              "      <td>19.671582</td>\n",
              "      <td>18.186143</td>\n",
              "      <td>18.279153</td>\n",
              "      <td>20.241733</td>\n",
              "      <td>40.370937</td>\n",
              "      <td>6818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-415.985906</td>\n",
              "      <td>98.632315</td>\n",
              "      <td>-11.634932</td>\n",
              "      <td>25.853185</td>\n",
              "      <td>-5.788481</td>\n",
              "      <td>-3.137197</td>\n",
              "      <td>-4.547056</td>\n",
              "      <td>-6.224032</td>\n",
              "      <td>-0.984390</td>\n",
              "      <td>-0.944783</td>\n",
              "      <td>-3.054848</td>\n",
              "      <td>-6.775986</td>\n",
              "      <td>-1.199266</td>\n",
              "      <td>-1.846986</td>\n",
              "      <td>0.013500</td>\n",
              "      <td>-4.888008</td>\n",
              "      <td>-3.797794</td>\n",
              "      <td>-2.617675</td>\n",
              "      <td>-3.850502</td>\n",
              "      <td>-0.216068</td>\n",
              "      <td>-3.617553</td>\n",
              "      <td>-2.801673</td>\n",
              "      <td>-1.148854</td>\n",
              "      <td>-2.278221</td>\n",
              "      <td>-2.267234</td>\n",
              "      <td>1.587220</td>\n",
              "      <td>1.155384</td>\n",
              "      <td>0.740378</td>\n",
              "      <td>3.569952</td>\n",
              "      <td>2.408959</td>\n",
              "      <td>3.202875</td>\n",
              "      <td>2.306133</td>\n",
              "      <td>2.023648</td>\n",
              "      <td>2.190643</td>\n",
              "      <td>2.170674</td>\n",
              "      <td>3.525891</td>\n",
              "      <td>2.949508</td>\n",
              "      <td>3.362839</td>\n",
              "      <td>3.350881</td>\n",
              "      <td>1.972878</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001296</td>\n",
              "      <td>0.001294</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>0.001749</td>\n",
              "      <td>0.001651</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.001043</td>\n",
              "      <td>0.001755</td>\n",
              "      <td>0.001912</td>\n",
              "      <td>0.000772</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>4.400634e-07</td>\n",
              "      <td>3.327332e-08</td>\n",
              "      <td>3.005347e-10</td>\n",
              "      <td>1.363162e-10</td>\n",
              "      <td>8.207327e-11</td>\n",
              "      <td>8.250479e-11</td>\n",
              "      <td>2.131331e-10</td>\n",
              "      <td>1.889226e-10</td>\n",
              "      <td>5.581513e-10</td>\n",
              "      <td>2.728019e-10</td>\n",
              "      <td>5.130721e-10</td>\n",
              "      <td>2.155240e-10</td>\n",
              "      <td>1.554211e-10</td>\n",
              "      <td>16.916067</td>\n",
              "      <td>16.995845</td>\n",
              "      <td>18.250172</td>\n",
              "      <td>17.788338</td>\n",
              "      <td>17.766173</td>\n",
              "      <td>19.751291</td>\n",
              "      <td>40.690395</td>\n",
              "      <td>6818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-389.216203</td>\n",
              "      <td>85.664793</td>\n",
              "      <td>-12.100713</td>\n",
              "      <td>36.519470</td>\n",
              "      <td>-5.123375</td>\n",
              "      <td>-2.776924</td>\n",
              "      <td>-13.039466</td>\n",
              "      <td>-9.157566</td>\n",
              "      <td>-0.670942</td>\n",
              "      <td>-5.634273</td>\n",
              "      <td>-4.250982</td>\n",
              "      <td>-7.122923</td>\n",
              "      <td>-0.542032</td>\n",
              "      <td>-0.231418</td>\n",
              "      <td>-1.115437</td>\n",
              "      <td>-8.437816</td>\n",
              "      <td>-0.201819</td>\n",
              "      <td>0.551886</td>\n",
              "      <td>-6.287000</td>\n",
              "      <td>1.212430</td>\n",
              "      <td>-1.322355</td>\n",
              "      <td>-1.636769</td>\n",
              "      <td>-0.759968</td>\n",
              "      <td>-0.810057</td>\n",
              "      <td>1.046202</td>\n",
              "      <td>2.115052</td>\n",
              "      <td>0.084705</td>\n",
              "      <td>0.850165</td>\n",
              "      <td>2.161932</td>\n",
              "      <td>1.031209</td>\n",
              "      <td>3.371500</td>\n",
              "      <td>1.195837</td>\n",
              "      <td>1.647626</td>\n",
              "      <td>2.550498</td>\n",
              "      <td>2.289650</td>\n",
              "      <td>4.546426</td>\n",
              "      <td>4.422887</td>\n",
              "      <td>5.207331</td>\n",
              "      <td>4.847171</td>\n",
              "      <td>1.290168</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007625</td>\n",
              "      <td>0.004986</td>\n",
              "      <td>0.004886</td>\n",
              "      <td>0.006394</td>\n",
              "      <td>0.005517</td>\n",
              "      <td>0.005415</td>\n",
              "      <td>0.005166</td>\n",
              "      <td>0.004006</td>\n",
              "      <td>0.003477</td>\n",
              "      <td>0.004232</td>\n",
              "      <td>0.003051</td>\n",
              "      <td>0.002426</td>\n",
              "      <td>0.002257</td>\n",
              "      <td>0.001646</td>\n",
              "      <td>0.001363</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>4.885682e-07</td>\n",
              "      <td>2.782180e-08</td>\n",
              "      <td>3.930282e-10</td>\n",
              "      <td>2.526319e-10</td>\n",
              "      <td>1.803548e-10</td>\n",
              "      <td>1.830779e-10</td>\n",
              "      <td>3.700877e-10</td>\n",
              "      <td>3.594756e-10</td>\n",
              "      <td>7.925989e-10</td>\n",
              "      <td>3.699298e-10</td>\n",
              "      <td>7.824133e-10</td>\n",
              "      <td>3.953022e-10</td>\n",
              "      <td>2.598654e-10</td>\n",
              "      <td>16.065135</td>\n",
              "      <td>17.536773</td>\n",
              "      <td>19.760998</td>\n",
              "      <td>18.659197</td>\n",
              "      <td>18.508388</td>\n",
              "      <td>19.315130</td>\n",
              "      <td>42.070370</td>\n",
              "      <td>6818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28473</th>\n",
              "      <td>-317.929720</td>\n",
              "      <td>102.891875</td>\n",
              "      <td>-11.296138</td>\n",
              "      <td>63.100710</td>\n",
              "      <td>3.179454</td>\n",
              "      <td>23.577949</td>\n",
              "      <td>-8.223133</td>\n",
              "      <td>9.402167</td>\n",
              "      <td>-5.556157</td>\n",
              "      <td>-3.244469</td>\n",
              "      <td>7.337272</td>\n",
              "      <td>-11.932577</td>\n",
              "      <td>5.764446</td>\n",
              "      <td>-7.471019</td>\n",
              "      <td>2.026820</td>\n",
              "      <td>2.039176</td>\n",
              "      <td>-3.724005</td>\n",
              "      <td>-4.037150</td>\n",
              "      <td>0.694880</td>\n",
              "      <td>-1.282598</td>\n",
              "      <td>-5.263950</td>\n",
              "      <td>2.131584</td>\n",
              "      <td>-3.103189</td>\n",
              "      <td>0.370581</td>\n",
              "      <td>-3.775483</td>\n",
              "      <td>-1.888006</td>\n",
              "      <td>-2.441419</td>\n",
              "      <td>-5.252970</td>\n",
              "      <td>-2.447790</td>\n",
              "      <td>-3.670099</td>\n",
              "      <td>-1.185963</td>\n",
              "      <td>-2.396619</td>\n",
              "      <td>-0.919336</td>\n",
              "      <td>-1.945775</td>\n",
              "      <td>-1.966704</td>\n",
              "      <td>-1.179681</td>\n",
              "      <td>-2.255737</td>\n",
              "      <td>-1.436406</td>\n",
              "      <td>0.067028</td>\n",
              "      <td>-1.359557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.087579</td>\n",
              "      <td>0.096757</td>\n",
              "      <td>0.095623</td>\n",
              "      <td>0.096657</td>\n",
              "      <td>0.096408</td>\n",
              "      <td>0.090893</td>\n",
              "      <td>0.082453</td>\n",
              "      <td>0.085329</td>\n",
              "      <td>0.080318</td>\n",
              "      <td>0.091796</td>\n",
              "      <td>0.088640</td>\n",
              "      <td>0.068943</td>\n",
              "      <td>0.052418</td>\n",
              "      <td>0.037552</td>\n",
              "      <td>0.023356</td>\n",
              "      <td>0.009747</td>\n",
              "      <td>0.003031</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>1.721884e-05</td>\n",
              "      <td>7.514108e-07</td>\n",
              "      <td>5.245616e-09</td>\n",
              "      <td>6.963911e-10</td>\n",
              "      <td>4.998469e-10</td>\n",
              "      <td>4.272372e-10</td>\n",
              "      <td>7.923524e-10</td>\n",
              "      <td>7.613523e-10</td>\n",
              "      <td>2.138717e-09</td>\n",
              "      <td>2.057230e-09</td>\n",
              "      <td>1.720287e-09</td>\n",
              "      <td>8.946828e-10</td>\n",
              "      <td>6.654127e-10</td>\n",
              "      <td>19.128398</td>\n",
              "      <td>14.495445</td>\n",
              "      <td>16.931463</td>\n",
              "      <td>15.204390</td>\n",
              "      <td>16.992092</td>\n",
              "      <td>19.320673</td>\n",
              "      <td>45.179566</td>\n",
              "      <td>1624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28474</th>\n",
              "      <td>-331.764251</td>\n",
              "      <td>124.405646</td>\n",
              "      <td>-14.785648</td>\n",
              "      <td>51.994524</td>\n",
              "      <td>6.264567</td>\n",
              "      <td>16.491336</td>\n",
              "      <td>-7.564639</td>\n",
              "      <td>6.345378</td>\n",
              "      <td>-3.357955</td>\n",
              "      <td>-5.134169</td>\n",
              "      <td>6.625129</td>\n",
              "      <td>-9.685062</td>\n",
              "      <td>4.320512</td>\n",
              "      <td>-8.031271</td>\n",
              "      <td>-1.888298</td>\n",
              "      <td>1.764179</td>\n",
              "      <td>-6.822305</td>\n",
              "      <td>-2.495925</td>\n",
              "      <td>2.173559</td>\n",
              "      <td>-1.585178</td>\n",
              "      <td>-4.163860</td>\n",
              "      <td>0.593757</td>\n",
              "      <td>-2.631176</td>\n",
              "      <td>-0.284521</td>\n",
              "      <td>-3.378616</td>\n",
              "      <td>-0.738132</td>\n",
              "      <td>-1.549109</td>\n",
              "      <td>-4.849086</td>\n",
              "      <td>-1.539207</td>\n",
              "      <td>-3.443830</td>\n",
              "      <td>-1.752096</td>\n",
              "      <td>-2.954752</td>\n",
              "      <td>-2.314675</td>\n",
              "      <td>-2.746039</td>\n",
              "      <td>-3.538315</td>\n",
              "      <td>-1.546050</td>\n",
              "      <td>-2.318102</td>\n",
              "      <td>-2.613273</td>\n",
              "      <td>-1.598695</td>\n",
              "      <td>-2.100652</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031036</td>\n",
              "      <td>0.026733</td>\n",
              "      <td>0.028357</td>\n",
              "      <td>0.028351</td>\n",
              "      <td>0.037423</td>\n",
              "      <td>0.034858</td>\n",
              "      <td>0.034519</td>\n",
              "      <td>0.029799</td>\n",
              "      <td>0.024647</td>\n",
              "      <td>0.023400</td>\n",
              "      <td>0.024329</td>\n",
              "      <td>0.017678</td>\n",
              "      <td>0.013911</td>\n",
              "      <td>0.010385</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>0.002703</td>\n",
              "      <td>0.000705</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>1.749313e-06</td>\n",
              "      <td>7.468346e-08</td>\n",
              "      <td>9.046450e-10</td>\n",
              "      <td>5.602231e-10</td>\n",
              "      <td>3.563160e-10</td>\n",
              "      <td>3.190903e-10</td>\n",
              "      <td>4.687838e-10</td>\n",
              "      <td>4.716414e-10</td>\n",
              "      <td>1.461288e-09</td>\n",
              "      <td>1.348944e-09</td>\n",
              "      <td>1.211319e-09</td>\n",
              "      <td>5.409722e-10</td>\n",
              "      <td>4.040735e-10</td>\n",
              "      <td>19.077424</td>\n",
              "      <td>14.326590</td>\n",
              "      <td>16.056080</td>\n",
              "      <td>14.861246</td>\n",
              "      <td>16.975874</td>\n",
              "      <td>20.143333</td>\n",
              "      <td>42.811203</td>\n",
              "      <td>1624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28475</th>\n",
              "      <td>-285.018059</td>\n",
              "      <td>113.076087</td>\n",
              "      <td>-22.463746</td>\n",
              "      <td>59.246327</td>\n",
              "      <td>-5.666200</td>\n",
              "      <td>21.617253</td>\n",
              "      <td>-9.790063</td>\n",
              "      <td>6.970939</td>\n",
              "      <td>-4.804441</td>\n",
              "      <td>-3.166981</td>\n",
              "      <td>5.600883</td>\n",
              "      <td>-14.433344</td>\n",
              "      <td>6.324796</td>\n",
              "      <td>-10.210112</td>\n",
              "      <td>2.017484</td>\n",
              "      <td>0.402096</td>\n",
              "      <td>-7.208714</td>\n",
              "      <td>-2.678059</td>\n",
              "      <td>0.936260</td>\n",
              "      <td>0.082816</td>\n",
              "      <td>-6.241912</td>\n",
              "      <td>2.937505</td>\n",
              "      <td>-3.490243</td>\n",
              "      <td>-3.152442</td>\n",
              "      <td>-3.099624</td>\n",
              "      <td>-1.964522</td>\n",
              "      <td>-3.471009</td>\n",
              "      <td>-5.265174</td>\n",
              "      <td>-1.956457</td>\n",
              "      <td>-4.629311</td>\n",
              "      <td>-1.342751</td>\n",
              "      <td>-2.114659</td>\n",
              "      <td>-1.566966</td>\n",
              "      <td>-2.516954</td>\n",
              "      <td>-3.847884</td>\n",
              "      <td>-2.567742</td>\n",
              "      <td>-3.263717</td>\n",
              "      <td>-3.042321</td>\n",
              "      <td>-2.146399</td>\n",
              "      <td>-2.098067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.057306</td>\n",
              "      <td>0.111258</td>\n",
              "      <td>0.066799</td>\n",
              "      <td>0.048367</td>\n",
              "      <td>0.045911</td>\n",
              "      <td>0.054256</td>\n",
              "      <td>0.064595</td>\n",
              "      <td>0.074933</td>\n",
              "      <td>0.077195</td>\n",
              "      <td>0.082777</td>\n",
              "      <td>0.076231</td>\n",
              "      <td>0.061536</td>\n",
              "      <td>0.053799</td>\n",
              "      <td>0.034548</td>\n",
              "      <td>0.021178</td>\n",
              "      <td>0.009391</td>\n",
              "      <td>0.002704</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>7.806029e-06</td>\n",
              "      <td>2.614661e-07</td>\n",
              "      <td>2.043550e-09</td>\n",
              "      <td>3.757870e-10</td>\n",
              "      <td>3.122230e-10</td>\n",
              "      <td>2.894953e-10</td>\n",
              "      <td>6.005570e-10</td>\n",
              "      <td>5.608500e-10</td>\n",
              "      <td>1.501216e-09</td>\n",
              "      <td>1.274367e-09</td>\n",
              "      <td>1.197984e-09</td>\n",
              "      <td>5.992238e-10</td>\n",
              "      <td>4.678283e-10</td>\n",
              "      <td>19.730442</td>\n",
              "      <td>14.473371</td>\n",
              "      <td>16.715092</td>\n",
              "      <td>15.132970</td>\n",
              "      <td>17.143333</td>\n",
              "      <td>19.685933</td>\n",
              "      <td>46.393260</td>\n",
              "      <td>1624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28476</th>\n",
              "      <td>-307.887370</td>\n",
              "      <td>122.095900</td>\n",
              "      <td>-23.212005</td>\n",
              "      <td>63.693595</td>\n",
              "      <td>3.971840</td>\n",
              "      <td>17.478439</td>\n",
              "      <td>-7.317237</td>\n",
              "      <td>7.306023</td>\n",
              "      <td>-4.563810</td>\n",
              "      <td>-4.416899</td>\n",
              "      <td>4.436400</td>\n",
              "      <td>-13.318176</td>\n",
              "      <td>6.339748</td>\n",
              "      <td>-9.212451</td>\n",
              "      <td>-0.483969</td>\n",
              "      <td>2.169782</td>\n",
              "      <td>-5.118673</td>\n",
              "      <td>-2.788817</td>\n",
              "      <td>1.254023</td>\n",
              "      <td>-0.103923</td>\n",
              "      <td>-5.580804</td>\n",
              "      <td>2.597125</td>\n",
              "      <td>-3.935530</td>\n",
              "      <td>-2.581285</td>\n",
              "      <td>-3.486404</td>\n",
              "      <td>-2.485742</td>\n",
              "      <td>-2.361087</td>\n",
              "      <td>-4.957199</td>\n",
              "      <td>-1.553510</td>\n",
              "      <td>-4.537924</td>\n",
              "      <td>-0.588247</td>\n",
              "      <td>-2.424859</td>\n",
              "      <td>-2.268741</td>\n",
              "      <td>-1.268100</td>\n",
              "      <td>-4.324460</td>\n",
              "      <td>-1.582233</td>\n",
              "      <td>-2.806122</td>\n",
              "      <td>-3.380400</td>\n",
              "      <td>-1.886348</td>\n",
              "      <td>-2.764482</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051341</td>\n",
              "      <td>0.048728</td>\n",
              "      <td>0.032641</td>\n",
              "      <td>0.025730</td>\n",
              "      <td>0.028475</td>\n",
              "      <td>0.032846</td>\n",
              "      <td>0.039428</td>\n",
              "      <td>0.031306</td>\n",
              "      <td>0.027777</td>\n",
              "      <td>0.023945</td>\n",
              "      <td>0.017192</td>\n",
              "      <td>0.017362</td>\n",
              "      <td>0.016552</td>\n",
              "      <td>0.010208</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>0.002627</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>3.237523e-06</td>\n",
              "      <td>1.237039e-07</td>\n",
              "      <td>1.059544e-09</td>\n",
              "      <td>3.952201e-10</td>\n",
              "      <td>2.259270e-10</td>\n",
              "      <td>2.105528e-10</td>\n",
              "      <td>3.336646e-10</td>\n",
              "      <td>3.855448e-10</td>\n",
              "      <td>1.360701e-09</td>\n",
              "      <td>1.154909e-09</td>\n",
              "      <td>1.149417e-09</td>\n",
              "      <td>5.152930e-10</td>\n",
              "      <td>3.240362e-10</td>\n",
              "      <td>19.835007</td>\n",
              "      <td>15.045454</td>\n",
              "      <td>17.441207</td>\n",
              "      <td>15.596128</td>\n",
              "      <td>17.268374</td>\n",
              "      <td>20.918048</td>\n",
              "      <td>44.952059</td>\n",
              "      <td>1624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28477</th>\n",
              "      <td>-314.205440</td>\n",
              "      <td>107.470410</td>\n",
              "      <td>-4.372862</td>\n",
              "      <td>63.332396</td>\n",
              "      <td>0.602186</td>\n",
              "      <td>21.512506</td>\n",
              "      <td>-6.431591</td>\n",
              "      <td>8.618725</td>\n",
              "      <td>-4.272436</td>\n",
              "      <td>-5.199386</td>\n",
              "      <td>7.306297</td>\n",
              "      <td>-12.762656</td>\n",
              "      <td>4.177597</td>\n",
              "      <td>-6.448019</td>\n",
              "      <td>0.846291</td>\n",
              "      <td>1.026296</td>\n",
              "      <td>-4.292827</td>\n",
              "      <td>-1.339958</td>\n",
              "      <td>0.999061</td>\n",
              "      <td>-0.214650</td>\n",
              "      <td>-3.707019</td>\n",
              "      <td>1.612400</td>\n",
              "      <td>-3.672745</td>\n",
              "      <td>-1.516425</td>\n",
              "      <td>-3.042864</td>\n",
              "      <td>-2.127875</td>\n",
              "      <td>-3.237235</td>\n",
              "      <td>-4.443754</td>\n",
              "      <td>-1.508186</td>\n",
              "      <td>-3.258960</td>\n",
              "      <td>-0.756866</td>\n",
              "      <td>-2.540907</td>\n",
              "      <td>-2.085191</td>\n",
              "      <td>-2.304037</td>\n",
              "      <td>-3.041714</td>\n",
              "      <td>-2.485071</td>\n",
              "      <td>-2.392804</td>\n",
              "      <td>-2.145155</td>\n",
              "      <td>-1.368650</td>\n",
              "      <td>-1.210066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055587</td>\n",
              "      <td>0.079388</td>\n",
              "      <td>0.098808</td>\n",
              "      <td>0.193068</td>\n",
              "      <td>0.149726</td>\n",
              "      <td>0.224234</td>\n",
              "      <td>0.194682</td>\n",
              "      <td>0.206133</td>\n",
              "      <td>0.176826</td>\n",
              "      <td>0.145955</td>\n",
              "      <td>0.100877</td>\n",
              "      <td>0.076456</td>\n",
              "      <td>0.044953</td>\n",
              "      <td>0.026003</td>\n",
              "      <td>0.013665</td>\n",
              "      <td>0.006273</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>1.051515e-05</td>\n",
              "      <td>3.680181e-07</td>\n",
              "      <td>3.436854e-09</td>\n",
              "      <td>5.580703e-10</td>\n",
              "      <td>4.480111e-10</td>\n",
              "      <td>3.983541e-10</td>\n",
              "      <td>6.029211e-10</td>\n",
              "      <td>6.966104e-10</td>\n",
              "      <td>2.143013e-09</td>\n",
              "      <td>2.271861e-09</td>\n",
              "      <td>2.033969e-09</td>\n",
              "      <td>1.111606e-09</td>\n",
              "      <td>8.762626e-10</td>\n",
              "      <td>19.916155</td>\n",
              "      <td>14.168335</td>\n",
              "      <td>16.746456</td>\n",
              "      <td>15.496058</td>\n",
              "      <td>17.377188</td>\n",
              "      <td>18.809463</td>\n",
              "      <td>45.100379</td>\n",
              "      <td>1624</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28478 rows × 188 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0           1          2  ...        185        186  speaker_id\n",
              "0     -378.984647  116.162422 -17.332546  ...  21.324619  40.286322        6818\n",
              "1     -360.585734  109.734169 -28.806914  ...  20.290513  41.420199        6818\n",
              "2     -385.111883  103.916242  -9.390188  ...  20.241733  40.370937        6818\n",
              "3     -415.985906   98.632315 -11.634932  ...  19.751291  40.690395        6818\n",
              "4     -389.216203   85.664793 -12.100713  ...  19.315130  42.070370        6818\n",
              "...           ...         ...        ...  ...        ...        ...         ...\n",
              "28473 -317.929720  102.891875 -11.296138  ...  19.320673  45.179566        1624\n",
              "28474 -331.764251  124.405646 -14.785648  ...  20.143333  42.811203        1624\n",
              "28475 -285.018059  113.076087 -22.463746  ...  19.685933  46.393260        1624\n",
              "28476 -307.887370  122.095900 -23.212005  ...  20.918048  44.952059        1624\n",
              "28477 -314.205440  107.470410  -4.372862  ...  18.809463  45.100379        1624\n",
              "\n",
              "[28478 rows x 188 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxcDmJCNAY4K"
      },
      "source": [
        "import pandas as pd\n",
        "root = \"/content/content/drive/My Drive/reddragon/spectrograms/\"\n",
        "train_df = pd.DataFrame(columns=total.columns)\n",
        "test_df = pd.DataFrame(columns=total.columns)\n",
        "test_pct = 0.1\n",
        "for classi in classes:\n",
        "  \n",
        "  class_exs = total[total['speaker_id'] == int(classi)].reset_index(drop=True)\n",
        "  num_exs = class_exs.shape[0]\n",
        "  train_num = int((1-test_pct) * num_exs)\n",
        "  \n",
        "  train_exs = class_exs.iloc[:train_num]\n",
        "  test_exs = class_exs.iloc[train_num:]\n",
        "  if test_exs.shape[0] == 0:\n",
        "    print(num_exs)\n",
        "    print(train_num)\n",
        "    print(classi)\n",
        "\n",
        "  train_df = pd.concat([train_df, train_exs])\n",
        "  test_df = pd.concat([test_df, test_exs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9DF0EL7BhJV"
      },
      "source": [
        "train_X = train_df.iloc[:,:-1].values\n",
        "val_X = test_df.iloc[:,:-1].values\n",
        "\n",
        "train_y = pd.get_dummies(train_df.speaker_id).values\n",
        "val_y = pd.get_dummies(test_df.speaker_id).values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOuQkEGaCQqa"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "train_X = ss.fit_transform(train_X)\n",
        "val_X = ss.transform(val_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPaMHxJyC-Hr",
        "outputId": "0d450e23-9dee-4b39-c434-9c2ef8da732b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "model = build_tabular(train_y.shape[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 187)]             0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 193)               36284     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 193)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 128)               24832     \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 150)               19350     \n",
            "=================================================================\n",
            "Total params: 96,978\n",
            "Trainable params: 96,978\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AyyjcuKDdUB",
        "outputId": "0800535c-a5fd-4e01-942e-9ae7772565d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(train_X, train_y, batch_size=256, epochs=100, validation_data=(val_X, val_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 4.4694 - accuracy: 0.0818 - val_loss: 3.0642 - val_accuracy: 0.4400\n",
            "Epoch 2/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.6998 - accuracy: 0.3498 - val_loss: 1.2773 - val_accuracy: 0.8137\n",
            "Epoch 3/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.6045 - accuracy: 0.5758 - val_loss: 0.6302 - val_accuracy: 0.8923\n",
            "Epoch 4/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0800 - accuracy: 0.7024 - val_loss: 0.3592 - val_accuracy: 0.9506\n",
            "Epoch 5/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.7854 - accuracy: 0.7806 - val_loss: 0.2323 - val_accuracy: 0.9641\n",
            "Epoch 6/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6217 - accuracy: 0.8230 - val_loss: 0.1599 - val_accuracy: 0.9770\n",
            "Epoch 7/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.8555 - val_loss: 0.1174 - val_accuracy: 0.9804\n",
            "Epoch 8/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8786 - val_loss: 0.0931 - val_accuracy: 0.9820\n",
            "Epoch 9/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3552 - accuracy: 0.8969 - val_loss: 0.0777 - val_accuracy: 0.9837\n",
            "Epoch 10/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.9068 - val_loss: 0.0682 - val_accuracy: 0.9877\n",
            "Epoch 11/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2792 - accuracy: 0.9230 - val_loss: 0.0537 - val_accuracy: 0.9910\n",
            "Epoch 12/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.9269 - val_loss: 0.0532 - val_accuracy: 0.9888\n",
            "Epoch 13/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2337 - accuracy: 0.9316 - val_loss: 0.0446 - val_accuracy: 0.9899\n",
            "Epoch 14/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2068 - accuracy: 0.9395 - val_loss: 0.0402 - val_accuracy: 0.9927\n",
            "Epoch 15/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9436 - val_loss: 0.0360 - val_accuracy: 0.9927\n",
            "Epoch 16/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.9461 - val_loss: 0.0345 - val_accuracy: 0.9916\n",
            "Epoch 17/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1670 - accuracy: 0.9518 - val_loss: 0.0319 - val_accuracy: 0.9933\n",
            "Epoch 18/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1450 - accuracy: 0.9582 - val_loss: 0.0316 - val_accuracy: 0.9927\n",
            "Epoch 19/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.9565 - val_loss: 0.0331 - val_accuracy: 0.9921\n",
            "Epoch 20/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1435 - accuracy: 0.9583 - val_loss: 0.0327 - val_accuracy: 0.9916\n",
            "Epoch 21/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1333 - accuracy: 0.9602 - val_loss: 0.0252 - val_accuracy: 0.9944\n",
            "Epoch 22/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9664 - val_loss: 0.0248 - val_accuracy: 0.9938\n",
            "Epoch 23/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.9625 - val_loss: 0.0276 - val_accuracy: 0.9938\n",
            "Epoch 24/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1144 - accuracy: 0.9663 - val_loss: 0.0271 - val_accuracy: 0.9938\n",
            "Epoch 25/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9698 - val_loss: 0.0248 - val_accuracy: 0.9944\n",
            "Epoch 26/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9706 - val_loss: 0.0264 - val_accuracy: 0.9944\n",
            "Epoch 27/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.9702 - val_loss: 0.0265 - val_accuracy: 0.9949\n",
            "Epoch 28/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9706 - val_loss: 0.0248 - val_accuracy: 0.9949\n",
            "Epoch 29/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0936 - accuracy: 0.9710 - val_loss: 0.0208 - val_accuracy: 0.9949\n",
            "Epoch 30/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9743 - val_loss: 0.0209 - val_accuracy: 0.9955\n",
            "Epoch 31/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9748 - val_loss: 0.0189 - val_accuracy: 0.9949\n",
            "Epoch 32/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9749 - val_loss: 0.0226 - val_accuracy: 0.9938\n",
            "Epoch 33/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9746 - val_loss: 0.0196 - val_accuracy: 0.9944\n",
            "Epoch 34/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9778 - val_loss: 0.0228 - val_accuracy: 0.9938\n",
            "Epoch 35/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9775 - val_loss: 0.0258 - val_accuracy: 0.9933\n",
            "Epoch 36/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0703 - accuracy: 0.9789 - val_loss: 0.0187 - val_accuracy: 0.9944\n",
            "Epoch 37/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9781 - val_loss: 0.0160 - val_accuracy: 0.9955\n",
            "Epoch 38/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9779 - val_loss: 0.0152 - val_accuracy: 0.9949\n",
            "Epoch 39/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9785 - val_loss: 0.0178 - val_accuracy: 0.9955\n",
            "Epoch 40/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9802 - val_loss: 0.0193 - val_accuracy: 0.9961\n",
            "Epoch 41/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9800 - val_loss: 0.0145 - val_accuracy: 0.9955\n",
            "Epoch 42/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9820 - val_loss: 0.0167 - val_accuracy: 0.9955\n",
            "Epoch 43/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9823 - val_loss: 0.0180 - val_accuracy: 0.9955\n",
            "Epoch 44/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9824 - val_loss: 0.0180 - val_accuracy: 0.9949\n",
            "Epoch 45/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9809 - val_loss: 0.0138 - val_accuracy: 0.9972\n",
            "Epoch 46/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0190 - val_accuracy: 0.9955\n",
            "Epoch 47/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9828 - val_loss: 0.0152 - val_accuracy: 0.9966\n",
            "Epoch 48/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9823 - val_loss: 0.0181 - val_accuracy: 0.9949\n",
            "Epoch 49/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9826 - val_loss: 0.0220 - val_accuracy: 0.9949\n",
            "Epoch 50/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9851 - val_loss: 0.0215 - val_accuracy: 0.9949\n",
            "Epoch 51/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9823 - val_loss: 0.0163 - val_accuracy: 0.9955\n",
            "Epoch 52/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9860 - val_loss: 0.0166 - val_accuracy: 0.9961\n",
            "Epoch 53/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9839 - val_loss: 0.0150 - val_accuracy: 0.9961\n",
            "Epoch 54/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9836 - val_loss: 0.0144 - val_accuracy: 0.9955\n",
            "Epoch 55/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0490 - accuracy: 0.9851 - val_loss: 0.0125 - val_accuracy: 0.9966\n",
            "Epoch 56/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.9871 - val_loss: 0.0137 - val_accuracy: 0.9966\n",
            "Epoch 57/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9860 - val_loss: 0.0197 - val_accuracy: 0.9961\n",
            "Epoch 58/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9856 - val_loss: 0.0157 - val_accuracy: 0.9961\n",
            "Epoch 59/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.0134 - val_accuracy: 0.9961\n",
            "Epoch 60/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.0142 - val_accuracy: 0.9966\n",
            "Epoch 61/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9862 - val_loss: 0.0151 - val_accuracy: 0.9966\n",
            "Epoch 62/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0140 - val_accuracy: 0.9961\n",
            "Epoch 63/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.9885 - val_loss: 0.0150 - val_accuracy: 0.9955\n",
            "Epoch 64/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.9867 - val_loss: 0.0115 - val_accuracy: 0.9961\n",
            "Epoch 65/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9846 - val_loss: 0.0188 - val_accuracy: 0.9955\n",
            "Epoch 66/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9875 - val_loss: 0.0172 - val_accuracy: 0.9944\n",
            "Epoch 67/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0461 - accuracy: 0.9869 - val_loss: 0.0184 - val_accuracy: 0.9961\n",
            "Epoch 68/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0405 - accuracy: 0.9872 - val_loss: 0.0147 - val_accuracy: 0.9966\n",
            "Epoch 69/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0436 - accuracy: 0.9865 - val_loss: 0.0123 - val_accuracy: 0.9966\n",
            "Epoch 70/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9899 - val_loss: 0.0138 - val_accuracy: 0.9961\n",
            "Epoch 71/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9894 - val_loss: 0.0138 - val_accuracy: 0.9966\n",
            "Epoch 72/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.9900 - val_loss: 0.0165 - val_accuracy: 0.9966\n",
            "Epoch 73/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.9887 - val_loss: 0.0159 - val_accuracy: 0.9961\n",
            "Epoch 74/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9889 - val_loss: 0.0143 - val_accuracy: 0.9966\n",
            "Epoch 75/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0375 - accuracy: 0.9885 - val_loss: 0.0139 - val_accuracy: 0.9966\n",
            "Epoch 76/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.9882 - val_loss: 0.0207 - val_accuracy: 0.9961\n",
            "Epoch 77/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.9886 - val_loss: 0.0180 - val_accuracy: 0.9955\n",
            "Epoch 78/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9885 - val_loss: 0.0193 - val_accuracy: 0.9949\n",
            "Epoch 79/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 0.9894 - val_loss: 0.0151 - val_accuracy: 0.9955\n",
            "Epoch 80/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0376 - accuracy: 0.9871 - val_loss: 0.0155 - val_accuracy: 0.9955\n",
            "Epoch 81/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.0239 - val_accuracy: 0.9955\n",
            "Epoch 82/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 0.0197 - val_accuracy: 0.9955\n",
            "Epoch 83/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.9870 - val_loss: 0.0216 - val_accuracy: 0.9961\n",
            "Epoch 84/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9885 - val_loss: 0.0169 - val_accuracy: 0.9972\n",
            "Epoch 85/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.9915 - val_loss: 0.0227 - val_accuracy: 0.9955\n",
            "Epoch 86/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9895 - val_loss: 0.0183 - val_accuracy: 0.9966\n",
            "Epoch 87/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.9892 - val_loss: 0.0173 - val_accuracy: 0.9966\n",
            "Epoch 88/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.9890 - val_loss: 0.0165 - val_accuracy: 0.9961\n",
            "Epoch 89/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.9913 - val_loss: 0.0179 - val_accuracy: 0.9955\n",
            "Epoch 90/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9905 - val_loss: 0.0201 - val_accuracy: 0.9955\n",
            "Epoch 91/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 0.0221 - val_accuracy: 0.9938\n",
            "Epoch 92/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 0.0258 - val_accuracy: 0.9944\n",
            "Epoch 93/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0346 - accuracy: 0.9889 - val_loss: 0.0260 - val_accuracy: 0.9944\n",
            "Epoch 94/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.9895 - val_loss: 0.0173 - val_accuracy: 0.9966\n",
            "Epoch 95/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.9914 - val_loss: 0.0168 - val_accuracy: 0.9955\n",
            "Epoch 96/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.9903 - val_loss: 0.0137 - val_accuracy: 0.9961\n",
            "Epoch 97/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.9907 - val_loss: 0.0173 - val_accuracy: 0.9961\n",
            "Epoch 98/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 0.9896 - val_loss: 0.0128 - val_accuracy: 0.9966\n",
            "Epoch 99/100\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 0.0152 - val_accuracy: 0.9955\n",
            "Epoch 100/100\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.9905 - val_loss: 0.0200 - val_accuracy: 0.9955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loNkH1DCLmEX",
        "outputId": "545bbdc7-475b-4e2a-b3ba-f39dd818b0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# Check out our train accuracy and validation accuracy over epochs.\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Set figure size.\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Generate line plot of training, testing loss over epochs.\n",
        "plt.plot(train_accuracy, label='Training Accuracy', color='#185fad')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy', color='orange')\n",
        "\n",
        "# Set title\n",
        "plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
        "plt.xlabel('Epoch', fontsize = 18)\n",
        "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
        "plt.xticks(range(0,100,5), range(0,100,5))\n",
        "\n",
        "plt.legend(fontsize = 18);\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAIBCAYAAABz4sjiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxkdX3v/9enqvd9erpnn2GGYWAYEFEGUAQFFQX3fbteISpqIonJNXr9qVGjiTGJJjFxC264K9fEFaOgOGwCMqDDMgPDMPvae/f0Wl1V398f31Pdp6urqru6q7q6e97Px6Mep6rO9j2nzjn1Od/zXcw5h4iIiIiIFEak1AkQEREREVlMFGCLiIiIiBSQAmwRERERkQJSgC0iIiIiUkAKsEVERERECkgBtoiIiIhIASnAlpIzs8vNzJlZwduMNLNrg2XvL/SyT3Vm9rFg324rdVqmK9exNtvjsJjH8TTXr2Nd5q1Snx8ykZntD36Pa0udlsVKAfYpInVhm+Hr2lKnXxYHM/tycEx1mlllHvM9Ecz302Kmbz4ys/XBzczHSp2WuWJmPwhdf/6+1OmRxS10czrd1/pSp1nmv7JSJ0DmzIks39cBtVNMM1T45EwwCDxepGX3Bss+UqTlS36+CrwdaAZeDtw01Qxm9hzgjND8xVLM43A21gMfDd5/LMd0i+JYN7OlwCtCX11jZh9xziVKlSY5pXQAUx1rOhZlSgqwTxHOuRWZvg9yxT6aa5pic879HthcpGX/CPhRMZYt+XPO3WtmO4EtwJ8wjQA7mA78DeDNRUxb0Y7DubCIjvU3AxXAL4CzgI3AC4PPIsV2oXNuf6kTIQufioiIyFxL5UK/wMxW55rQzOqB1wQfv+mcixc1ZTIfvC0YfhP4Vtp3IiILggJsySlU5uxyM1tmZv9iZrvNbDBcWcXMaszsjWb2TTP7o5m1m9mImR01sx+b2dU51pGr4tmEiltmdoGZ3WRmx4Ll7w3StCTLsrNW/EqvpGdmzzOzm4O0D5vZLjP7qJlVTbGPXm5mt5lZj5n1m9kOM3u/mZXPpiKgmS0xs7cF2/uwmXUF6TpgZt81s2fkmLdQ23a1md2aadvy3Z6QbwGj+OvPtVNM+3rGizB9LUjTjI+1XHIdh6FpNpvZd8zseLAf95rZf5jZ8imWXW5mLzOzG8xse3D8xsyszcx+FWyPZZhvP/Db0Of0sqA3hsZNWcnRzDaa2RfNl2kfMrM+M3vQzD5iZg3T2S9mdoaZfc3MDgX7/bD5svU5b5amw8wuBJ6CL+7yE3yQ7YCXmlnrNOZfa2b/FBwXvcE2PmlmPzGzt2Q73s3sYjP7upntMX9t6zOzncF2vjBt2uns5/WWpbxu+vxmdkVw3B4zs0Tab7rBzP6vmf3S/HV3IDgPd5rZv5nZukLtEzO7KkhX3MxWTbHMO9OPv3yZ2VYz+2Gw3cPBvv9nM2vKMO33g/XlfIoRHJvJYNrLZ5q2fJnZtmCdHzOzCjP7gJk9FPxe3eavoVNel8zsVWb2czM7EVwfTgSfXzmNeZcG5/F9Nv5fsd/MbjGzPzWzxhzzVpjZ+8xf3weC4+Q2M7sq330hIc45vU7hF75Mp/OHQsbxLni9HTgevB8C+sLz4AOl1LRJoAcYCH3ngE9nWcfl2dIQWu5+4E1ALPjcgy8Hl1r2I0BdrvlzbPs24H1BupNAdzBMLfs2IJol7Z9O28ZufPDogNuBv0+tYza/DRAHuoDhtP38F1PMO5ttC68/07Z9chbb9sNg3iemmO7uYLq75+BYyzouGH9V2v4/iT8XHHAUX5RlymUHr16Ccyj0ugmIpM13f/C7p6Y5nvb67HSO9WD869LS35f2+SBw9hRpvyLY7tT8o6FxR4DVs7wefSlY1g2h724Pvvs/U8z7v0O/hwNG8OVpw2k8P22eKPDZtN+hP9jnqfOkZ7rXlNA060PLW59tfuA94fXgr283hqbdlmF7wte9HuDSQuwTwIC9wXcfzrHMzaF5L8njtw0fRy8P0pI6F0ZC4/Zn2GepeRPAuhzr+FQw3eN5HnfXkuX3mub8qd/pk8AdwftR/DUzfGx9LMv8FcD3Q9MlgmMw/Ft/FyjPMv8LmHidGA1+51jou1ekzbM/+P564N7gfYzx8zt1fX3rbM7pU/lV8gToVeIDYPoB9kngMeC5BEEAcGZoupcD/ww8C6gJfb8S+EjoRH9ZhnVcni0NoQvfAD4Y+DKwNhhXA7w7tOyP55h/f45t7w4uZJ8EWoJxDcDfhrZ/0kUGeENo/HcIggugCrgO/8eWuuhtm8Fv844gjRcAFcF3BmwA/i24+MWBpxVh214WGn9TaJ9XA3+G/0NM/XnMZNuuDi3/2VmmOStTGot4rOUatwYfCDhgB3BR8H0EH3gfCu2PTPNfhA8enw80hL5vBv4itOxJN0y50pXHsf700H65C3hKKP0vxd8gOGAPaTeqTAyMuvA5y5uDcRX4wD11s/DNfI+F0HpqQvvh0tD3bw2+eyTHvC9mPFC9C7iU8etURfD5BmBL2nz/GNq2rzLxmtYYHGvfn+5+Dk2zPrTc9VnmH8Kfv19n/PyKAhtD0/4b/nzbFNqesuB4+h/Gb2yqC7FPgP8bTL8PsCzb9plgmofz/H3Dx1EP/snM2aFteh3j18vfk3bjD+wMxv1tluWXM54J9N4803YtWX6vac6/LbRdw8A7gapg3Frg/4WWn+m6lMqoSQIfB5qC75cwnknjgE9lmPdpjN9EPYK/tpaHjqcLguU/L22+/Yyf04fxx3pqvrOAexj/72+c6Xl9Kr9KngC9SnwATD/A7gXWzGI9fx0s59cZxl2eLQ1pF74bsyw7dcGflBvK9AJsR/achf8Kxt+a9r0BTwTjbiHDn1Fa2rcV4bf7XLDsrxRy24Jxj6bSTVquajD+nbPZNnxgd2iK3zUV/Jwkw9OJIhxrucZ9IRjXASzLMP5cQrlFM9gfrwnm3ZNPuvI41lPB2BOEbkpC45/GeK7mX2dbP/6JR6bj4c+D8YNA2QyP57dk2gdAfbBcB1ycYb4yxnNe7yS4GZ3G+s5kPIfwH/NIZ9b9HJpmfWifrc8yvwP+ayb7KlhOFH+z54A3F2iftDKem/zCDOMrgfZg/J/nmd7wcfQ4mW8Knh+a5rVp494TfH+IDE/dgFcH44cJMhNm8Ju6YPvSnxSFX/+dYf5tofkzZVhEGH8S80jauNWhc++TWdKX+o+LASvTxt0ZjNtNHoEw4wH2MMENc4ZjIRW4/6+ZHqen8ktlsGW6vuWcOzyL+VOtPzzTzKIzXMbfZfn+J8HwDDOrmcFyR/B3+LmWfV7a9+cz3nTcJ11wRUrzDfxj92JJ7dNLc0yT97aZ2Xn4Vj4A/s45l8ww75eZRXNwwTJvDD6+xszq0tIQxT/eBrjJOdefx+ILcayF02L4suAAX3LOtaVP45x7BF/sZaZSad5oZgVtzSco05oqR/zPzrnB9Gmcc38A/jv4+MYci/tkluMhdSxV43NbZyJVkfFb4S+dcycZbx0lU2XHK/BPdQD+yjkXm+b6rsEHPp2MN4M41/5hpjM632zhL4OP6deAGe0T51w7/sYb/BO0dK8EWvCB17cyjJ+uf3bOTWr+1Tn3a+B3wcc3pI3+Bv5Gaw3wogzLvC4Y/rdzrmMWaWsBlud4NeeY9xD+icQEwTmT+v86x8yeEhr9avwN0TC+iEsmf4e/lpczXukbM9vE+G//Qedcb64Ny+KHzrnHMqS5HZ+LDZP//2QaFGDLdN091QRmttzM/tbM7jHfkUjcxitH7Qwmq8E/9spXl3NuT5ZxR0PvZ7LsR3MEcKllp19Unx4MRxn/Q5ggCLpvn0F6xpjZ6Wb2aTN7wHxFw0Ron6Yq/KzJsYiZbNvWYBjH545MEvxhbJt6C3L6Oj53pJbxADblanyRD8jQ9nWRj7V0GxjfR7flmC7XOMysPqhIdLv5yo2xUJrDQW+u33Mmno5/4gLw6xzT3RoMz7PslVjvy/J9+BzMFYBkZGZnAJfhj4dMgds3guEbMtxEXxIMjzvntuex2tR8tzrnhvOYr1CGgAenmsjMLjOzG83ssaCCowsdN+8PJks/Zma6T8AXZQJfsTS98m4qiL3JOdeT53LDpnMebQ1/GazvB2npAMDMTgOuDD7eMIt0AWxwzlmO1+U55t2WJbMF/LU01QpSeNtS7+93zvVlmtE51w1sT5sexn/nBP4p1UxkO6ch+3+ETIPawZbpmpRrF2Zmz8QHfOEa4P2MP9qN4nMGwAdU+eYwnMwxLtx020xat5jOstPPlVSLBp1T5A7NOJc3qDn+Pfxj2ZRwxbQKfABZO3nuMTPZtmXBsMM5N5Jj/tk80cA5t9d8KydX4MvZhgPptwbDx5xzE25g5uBYS7cs9D7X75l1f5jZmcBvmBgIDeLLbKZyhFPBTK7fcybyTX8Z/g91UsdTQW7yJM65uI03gjKTc/Ct+JuAu5xzezOM/zVBJUrgtYwH3ACpHP8Dea5zpvMVSmeWpwFjzOwfGQ+iwQdS3fiiAjDeUVj6MTPjbXPO3WET26r/VJCWM/DnKsB/5rvcNLmOw9S4ZRnGfSlI04vMbLVzLjXt2/EZho8757bNMm2zkXW7nHPDZtaJP8/D25Z6P9V/Rer8DM+b+p07nHMD+SQ0ZDr/EbNpNeqUpRxsma6sPVeZWRk+EGwC/oh/fNfgnKt3zi13vgObcJNyk5ojW8Cy5VbMivne7G7EB9e34csv1jjnGkP79LXFWPccSwXVlwRBKOabY3tJ8P3XwhMv4GPt6/jgej/+d1vqnKt1zi0L0hxu4m6+pHlOBMV4rgk+XmqTmyJ0+OtPah+lFxOZ6TlYlHM3Dzl7AzSzKxkPrr+Ab76w0jnX7JxbERw3/5qaPG322W5bKhf77TZ+5/T2YD2POOfuyTxbcTnfGdSD+Jvot8HY8ZPqjOrLpUhXCZX6GJYcFGBLITwTOA3/h/ES59z/ZMjpKkkvkUXUHgxbzKwix3QzbRv4RfjWPrqBlzrnbs9QZrFY+zT1tKJY2xb2X/hcXBjPtX4zPsckjm8HOawUx1r46U2ubc44zszWMv4o943OuR8657rSJivm+RFOf67iJ6lxqSYh58rVQM52l9NcFpQ9TTkeDE/Lc70znS+Vq5erDfmsbQ7nIVUG+VfOuXc75x5xk7uLz3bczHTbUr6Jf8KyEXhuUGTo2mDcbHOvYXrnUbanpqng/61mFsFfK1fjyyh/I8s8cyXrdplZJbA0+BjettT7qYqGpcaH5039zi1mVugnXzJLCrClENYGw/bQI7t0z5+rxMyRVNnJcsaDpwmCnJ9nz3D5qX36eKZKaYFi7dNUWb8yfLnYSYI/tstnu6Kg7Ot3g49vScuN+rlzLr2YQimOtX2MB5xX5JjuuVm+Xxt6/4cs0+RK81gxglBuYj4eDC3jeTmmS6Vhh3NudAbrmalUjvSP8C2G5Hqlzru3huZPFSFaYWYTyu1OITXflTZFh0tpuoPhsiBoyuTiPJaXTeq4yXjMBMdCtmNupvsEgKCy3PeCj+/AN+W4HF9u/Nv5Li+DXOdRaly2suPfxReVOw1febdQlRsL4Tk5ztHLGC+OF962sbLV2TqDCSoqj5XVDo1K/c5R/I2qzCMKsKUQUjWXl2eoFIOZrcG39buY/BHfZjDAB7JcVN/MzHOQUvv0zEx//mZ2Pr7jnYJzzj0E7Ao+figIptO9lcJVxksVE1kJ/A3+UTikFQ8JzPmxFlRauin4+C4za0mfxsy2EKrdnyZcs/+pGeatBz6cIwnhik+TermbSlA57FfBx/dlamnHzJ6Kb80AxgOrogt+w1RxoB845/pzvfDtCQNcE2oh5rf4JukA/nWKpy5hN+KfhCzFtws/XTtSyce3qpG+TdXAX+WxvGxSx82kYybwLuD0LONmuk/CUjnFr2C8qMpsKzem/HWW69oV+PbtYbxC4wRBWeNURdgPM96iyGwrNxbCOsaLO40JrqEfDD7udM49HBr9X/inIlX4dsgz+SC+uOAo4628EFT8vyP4+EnL0hurlIYCbCmEu/AdwRhwU6gsbdR8N8PbWGRlxYKgK9W01wuBb1jQvbCZVZnZ2/CPUruzLGIqt+BzHZuB71jQDbX5Lm1fF4zPVTlltj4UDK8AvhsErqltexe+De5C/NHinHsQf8MCPsAGOMZ4KylhpTrW/gG/v1uAW1O5gua9AF+DP9uThl2MN9f4NTO7IDUiqLC5jdytnexmvFJbuExsPj6M/3M+A/iVBc2EmVnEzF6E39dlwJMUpgjAdL0lWO8Q8PNpTJ+60VlJEFgFxSaux//ulwK/MbNLUzeGwTlzuZl9O7gRIphvD77DIoD3m9lXwkVPzKzBzF5vZqkmAlPzHcYfhwD/YmbPTwX7wW/7azJX0MtXqgm+q83sb1JFAMysycw+CPwHvonBSWa6T9KWsR14AF+ZOpUjX6hjYyVws5mdFaSnzMxew3hTlw8y3mxkJqng/xJ87m2pKzem9AJfNLPrbLwL+rX4m9ZUzvyEm+ngSdxng48fMN86UlMwb5OZfQLfGy/AvzjnjqWt8z34iu+bgLvNd3lfHswfNbMLzexLZrbYniLPf9kayNbr1Hgx/Y5mLp9iOe8KTeuY2I10O/4RY7aOFy7PlgYK2C1xjm3flmPZWdMWjP/X0LqT+KIEqQ5HfsN4d+K/nMFv8ykm7tOe0LL34nOws+23Qmzb36Wtv4vxDhHuYBZdpWdY1/Vp6/qHEhxrU+2PFzO5q/FUyyVTdZX+EiZ2Tz3AePfu/fiiG1nPNeArafMewFeY/HRomqzHejD+9UzskrqXid1oT9lV+hS/4bSuFWnz7Arm+WEe8zwQzPOjtO/fkvb7DDO9rtI/FxqfOp6ydpUezHc+471OumA/9gfvj+OD/7yvSWnTlTPe7Xb4+pLqHOfnwCfIcQ7OZJ+kzf+20HR59dyY6zjC9xqYupb1pKXxAL6pvKmWd2donrx6bsywrGtDy5qqo5njwOvT5t8WzPvJULpiTOy+3AGfyLL+CnyOfWq6mXSV3hOaNsb0u0q/Nsd+uTGY5sbZ7N9T9aUcbCkI59yX8AHINvwfTRm+2aH/wD/ifDjrzAuYc+6vgFfht/sk/jHeLnyOwwsZbz4r79xe59wH8H+Qv8f/gZfji6V8Et/z3tHsc8+ec+7D+MDwNnwwmdq2D+ADwul26DEd38H/yaZkKh6SSldJjjXn3M34NqW/j69oVIFvyu5z+N9jX455f44vj38z/lgow/8Bfh24wDn3mylW/278TVNq29bhix9NKq6SIw0/AM7B50I+if894/inBx8FznXO7cq+hMIys2cBm4OPN+WaNk1q2peEiwk5574ZLO/f8G2hx/Ed3xwAfozvuGjC9jnnEs656/G5vN/B32SU45+Q7MQXX3o1aZxzf8Tn6qaOhQj+9/w8PvjemT5PvpwvB/8CfPGV3fig2PDXgz8FXsYULZHMZJ+k+SGMPREq2JMN59xP8LnP/4U/7w1//nwGH/BnPZdCUsWFCl25caqOZpbj92EmMfy18YP43ior8TdivwFe7Jz7m0wzOedizrnX44uZ/Q/+yUR9MPwf4FXOuTe5LHUjnHO34HOw/x5fZn8I/99zBF887J1M0U6/FJ4FdykiUgRmdjf+j+QjzrlPlDo9IiLTZWavxgfZQ8AqV5jy1wVhZj/DZwB8zzlXlPooeaRlG/Ac4G+dcx8rZVpk/lAOtkiRmNlzGG9h5Je5phURmYf+PBh+b54F16czXrnxi6VMi0g2CrBFZsHMPm9m15rZilTls6BiyjuBnwST3eacuz/7UkRE5hczewc+VzYJ/EuJkzMmaCnji/j45T7n3J0lTpJIRuoqXWR2ngX8WfB+xMwG8U2ppVp62IkvRy0iMq+Z2TPw5cobGW8S8gvOuUdLlyrPzD6N7wV1Bb7+Qxz4y5ImSiQH5WCLzM5H8BXVduIr3NXjm+a7E98W7oUue4coIiLzSRW+8mw9vqWij1KYNr0LoQVfuTcG3ANc5Zy7t7RJEslOlRxFRERERApo0RURaWlpcevXry91MkRERERkkXvggQc6nHOt6d8vugB7/fr1bN++vdTJEBEREZFFzswOZPpeZbBFRERERApIAbaIiIiISAEpwBYRERERKSAF2CIiIiIiBaQAW0RERESkgBRgi4iIiIgUkAJsEREREZECUoAtIiIiIlJACrBFRERERApIAbaIiIiISAEpwBYRERERKSAF2CIiIiIiBaQAW0RERESkgEoWYJvZ18yszcweyTLezOzfzWyPmT1kZk+f6zSKiIiIiOSrlDnYNwJX5Rh/NbApeL0D+OIcpElEREREZFZKFmA75+4AunJM8nLgm867F2gys5VzkzoRERERkZmZz2WwVwOHQp8PB99NYmbvMLPtZra9vb19ThInIjJryTgkYqVOhYjMRHzIn8MiGZSVOgGF4Jy7AbgBYOvWra7EyRGZG85BchQSg5AYzndmP0980M8fHloUymogWjNxWN4I5Q1gVpTNmTrJSYh1w0jH+Mu5zGnFID4weduyDZMxiFb7ectqx5cTqQLLMx8iWpmWnlq/7JE26H0M+h6Dvl1+ePIJv+6qFVB7GtSug5p1/n15A8S6Jm7vcLtPc/UqP03NOj9P7WlQvRoi5fnsUEiMTL1vwvsxOZp5URbJ/DtYeWgb2kPb0gUkMy8rUjnxNxhbVp5/Vy4e2o7wNoxkmcHGj4H0dad+xwnfVQc3SBn2U+ocSl9GeQNUtvhXVas/p1LHVyIGQ4dh4AAMHPSv4WN+uem/S2Io+3ZHqiauN3X8TdgfoaHLEiBaWZbftMyvPz1dyRzXoGj1NPdplmHqeMBlP38LwSX9doSPm9Rvm37tGenw01gUataMn4up8zdSkeG474B4f5aVR9KOv6n2TbVfdyaVLcH1YY2/Hs3GSNfEa1bvYzB0FCqW+GM4dTxXtkB5k/8tMl1LXKbQzPnzMdNvGqkMzpOWiesoqwMy/AeZwbrXzm5bC2w+B9hHgLWhz2uC72SxcUkYPAIuMXmcmT9pcwV2LukDj8HgTykZm/oiFamcfqA43A5dD0L3A37Ytyv480kPAnJc8DKmOxQgpweD2QKZ8B96YjDzPiumSPnEi11li7/QJ4YmBxludOo/zvBvFKmEWM/kP7HUn1Ssy//WBWXjgWBiKEfwVchVRqFuIzRshlUv9vtv8JA/frt3wJGfTbxhsmiwr1vHhwP7oe0OGO0tfnrBB4HRWv/7Zzpvkokp9p9B5dLxY6Z2XZaAORT0j3TAYOpYH8j/WLeoT3P4eKtsDQKOTOd+MrjpHIDYsdB5NuBzKhODU68zUhEEs8kgzVMcrxb1+4UIDJ/w2x9WudQHFOHzpWIJRFdl2YbU/hvw58xg6Fph5ZPPveol2W/KkqPBed0Pw225z+vKlmns10G/jRNueoLrRvp2zyeRSv+bVjb746d6JTQ9JTiWl/r9M3DQn7/td8PgDybetIwFisH5W7WcjPvJBedQYtAH8xNu3IZy31TlUr1yPPiPVAXH8+D0/m9iPf44Cm9Lw5lQvcZfe7oe9OdpLEdp32hVcDOQJaMi0w119RL/Pz50FHp2+DRMlYlkEQXYefgpcL2ZfR+4GOh1zh0rcZokZaQLOn7nD/qGzVC/aXp3ysmEvwvufhC6goC1+w857uoDVjbxIlXR7C9CAwd9cJJ3YGRT56YkR33aBg+Pz1Z3BjSd49+nLk6DXaE/nzwDwEjF5ACgrMZ/nzHZWXKXo1Vk/nPLIVNuXbQaSE7OnYoP+Atqeo5Mz0M++Ajvu4olPudkLKdrcPxPOj4wMWDJlPOUHlA2bhl/n55jYpG0dKZuPJKTf89UTt6EHOq0G61k6E8unsopzOPP37mJOTjhG46KJdB4tg+uc50rzvl9PNoX5Ao1Zr8ZjPWO31gOHc0/CI1UZjj+qyf/4WULrNMl4+O/eeqPu6LZb3skj5vP+cgFT33CgUmkfGKOYjhYnXADnX4OpZ1HyXgo9zM1XBuc14ucC+VixgdCAWWGJykWCZ3P4Vz1aR6fU0p7ihGtzuu47egbZsfeTp7c9zix0RitrWtYs3wZ65fXsXJJNZHILNLokowMD3DgaBsHjrdzvKOL8ijUVJZRWxmltrKMmqoyaisjNES6aXDHqIgdxlLBf/cOfzyW1eCiNcStmhGaGWYlrqyc2sooVRVlTEhiWS00nAUNZ/v/+dr1E/bH4EicQx2DHGrr5XjbUfp62qisrKGutp6GhiU01tfT3FBNU20FDoiNJhmJJ4iNJonF/au6MkpzbQVL6iqorSrDsv2O8cGMTwCcc3T3xzjRO8zZM9+7RWEuY7b9HKzY7HvA5UALcAL4KFAO4Jz7kvm9/Dl8SyODwJ8457ZPtdytW7e67dunnEzyNXAA2u6C9juh/S7ofXTieItA7en+JGzc7AODkc7Jj7eHjozfiUerYcn50HwBNJ6bOejIVCxgpN0vu2JJ2h9S+C490+PuDHfuE3II0r4HaDoPmp/u07jkfKhoKu5+PtWEA7LkcFAMJUdAKVJizjlO9Ayz93g/tdVlrFlaQ3NdRfbAoAScczx5vJ/7dndw3+5OHtjTxfBoguqKKFXlUaor/bCqIkrSOQaG4wyMxBkaSTAwHGdwJEEkAi0NVSxrrKS1oYrWxkqWNVbRUFNOMumIJx2J4BVPJHEOqsqjVJZHqAyGVcH6yqJGWTTih5EIZWVGWcQYTTgfaI0mGIkniY0mGU0kSSYdZoaZzzZIvR+OJegbHKU3ePUNxugdHKUsEuHM1fWcuaqBTavqOX15HeVlE3NME0lH18kR2nqH6egboWdglJ6BWPAapXcgRv9wnHWttZy5up7NqxvYtKqB6oropH3bNzjK0a4hjnYNsfNQLzv2d/PHfd0c6fT/bWYQjRjxxHh8VVkWYW1rLSuaqiiLGuXRCGVlEcoift+Uh/ZReTRCNBgmko69x0/yxLGTHGgbIJlHyFYeNRprK2iqraCptpyR0SQdfcO0941MSFt4+rUttaxrreG0ZbU011UyHEswFEswOBIfG/YNjnKwY5ATPcOT5h/NsNx80tsUBNvN9a2oT9YAACAASURBVJUsra9gaX1l6FVBPOnYd6Kfvcf72RsM+4fjlEeNfV9+BdHZ3MTMkJk94JzbOun7UgXYxaIAu0Bi3XD8Njj2Kzh+iw+wwRfVaLkEll0GrZdCWX1QPitURqtvt8+RiFROznGsWgnNT4MlT/fB+ELP1RI5RRzpHKTz5AgbltdRX51PWe/MkknH8Z4h9p0YYN+Jfva3DXCwfYCKsgjN9ZU011XQXF9Bc10lzfUV1FSWUVkWoaI8QkVZlKryCBVBEDWacIwmkowGuWKj8SS9g6Mc7hzkSOdgMBziSOcgvYOjtDZWsqKpmmWNVaxYUsXypipaGiopi0x+jD2aSLK/bYAnjvbxxFEf6JwcmlhuuaYyytqWWtYsrWZtSy2nLavl9OV1nL6ijnWttWPpTG333hP9/HGfD8p27OvmWPdQkBNZRk2QG1lbVUZlecRvW2q7gm1MJB3VFVHqqsqprSqjrrqMuqoyyqMRHjnYw327O+no80/1musruGjTUhpryhmOJRmKxRkeTTIUSzAUixONmF9fZZlPQ1WUmsoy4glHexCMtfcO0947Qv/w/KnQV10RpbGmnMbaCoZjCQ52DIwV8y2LGqcvr2PFkmo6+3xQ3XlyJGtwWlURpammnOrKKIc7BseCRDNYv6yWTSvrGR5NcqRzkGPdQwyOTHxStH5ZLedvWMJT1y/hqacv4bzTmqgsj3C0a4j9bQPsb+vnQNsA+9sGaO8dJp70x2s84YJXMuPnRJCO9cvr2LSynk2rgtfKek5fUUcy6Tg5HOfk0Cj9Q3H6hkY5GboB6emP0TMYo6ff30xUlkdobfTH+rLGKlqDYTzpONg+wIH2gfFh2yDdAzGqKqJUV0SpqYxSXeGPz/rqcta11LCu1R/rp7XWsq61lqX1FcTiSbr7Y3T1x+g6OUJXf4zu/hiRiPnzNziHK8uilJdFGByJ0zMQG5unu3/8fefJETpPjtDdH5tQhDtisKalxp9jy+s4fUU9G5bX8uxzllEWnfu2OxRgS3bJhH+83L8X2rb5oLrzPp97XN4Ay58Hy6/wQXXjU6YOipMJnxsZrVFOpMgMJJOOwZE4leXRSTlxM9XeO8xjR/rYdaiPxw73svvoSZbUVXDuukbOPa2J805rYk1LzVhObOfJEX63q507d7Zz16429p0YGFvWssZKNiyvY+MK/+e2urmayvJo8McZoaI8OhZUtvcOc6x7iOPdwxzvHuJY9xDHuoc50D7AcGw8UKkoi7BmaQ3xZJKuk7GCBnNL6ytZ01LN6uYaGmvKae8b4XjPEG09PoCczt/gssZKNq1qYNPKes5c5QOcoVjCPyLvGORQxwCHOwY52D5I39B4mdaIwbrWWjYsryMWT/LQ/u6xAL26IspT1jdxWmstw7EEAyPxsdzkweEEw6MJyqMRyoPApLzMqIhGsIiN5Tb3D4/SH+Q6A6xtqeHiM1u4+MylXHxmC2esrCtY7vrgiA/mopEI0YhRFjWiEf8yM2KjCYZHk4yMJhgJhsOjCUaDoDEe3AglgmF5NHXD5AOu1Huz8TpxzjlfisQ5qoKguqGmYsJNSyptTx7vZ/eRPnYfPcnjR/ro6BuhpaFyQkCZCjCX1Plc3caaCqpCudSj8ST72vrZfcQv47HDfew5fpLayjJWLqlm1dJqP2yuZsWSajatrGdJXZYifQtcMulmV6ylQBJJR3f/CJ0nY0TMWNdaQ2X5/MmcU4B9Khs9OV4JY+Cgz41OvR886MsYj5XdNFh6Iax8Iax4AbRcnGfLBCKLU89AjH0n+tl3wufyDI8mJgUM8YQjkUySdP7PKekcyaT/g6ivLmN5U1Xwqh7LOR2NO3Yf7ePxIDDYHeSUpgKmsqiNPc6vroiOPXavCj2KrwwC2qRzPpiJJ4kHj+6HR5PsPd5P58nxegrN9RWctaqBrv4YTxztG8vZa6ot55x1TfQOxHjkoK9AWVdVxiWbW3jW2ctY1Vwd7IN+njzRz77j/bT3Ta/+Q8RgWWMVy5dUsXJJNeuX1bF+mQ88NyyvY1Vz9YTHuyOjiQk5YYMjCWJxH7Slym8Oj/p9VDEhCI1QHo1QFxTfWNVcTU1l9upG8USS9r4ROvpGSGb4P4yYsbalhqba6QdRXf0j7Ds+wJMnTrIv9Sj7RD/RiI3ldJ6/YQmbVtUXLMctkXSMjCZybquIFJ4C7FOBc0HrAnf6stJd26F/P4z2TJwu1axQuJmvVNNCSy8MarWLlJZzjv7hOG29w5RFfMBUX10+KecqH8OxBEe6fI7j4bGcR1/0IWJMKANZFo3gnH90uu/EAN0DEytkRiNGedSIBuUno5HIhBy9iBmRiA/QzIz+oVHaeodzlqFc0VTFmasbOHNVPSuXVDMc5ACOPdoPhrHRZDBuYm5hNGKUZyjXuX5ZLZvXNLJ5TQNnr2mgtXG88txQLMFjh3t5+EAPDx/o4dGDvdRUlnHp2a1ctmUZT93QlDMI7Bsc5Vj3UFCWNqjEFLx3uKAYRjWtDZUleXwrIlJM2QJs3eoudLFu2P/d8aB66Kj/vrwRll7sy0uHA+jadb4ctMo+yxwaGI7zh71dbN/TxeNH+gDGAsBUZZ6IGZ0nRzjeM8yJHl+kYCg2uVWMyrIIddXl1FeX+SIUoWWklplIuqBCji9n6of+kXpYNGKsavZlcX3ubyo32udEA6xZWsNLL1rNhmV1rF/uc1zXtdZOqvw0HYmko6NvmBM9/nWse2isgtamlfU05pFLWijVFVGednozTzu9eUbzN9SU01Cjp1wiImEKsBeygQPw2xdC3+O+k4llz4bWoPJh07n5d5AhpzTnHEe7hug4OcLQiM85TQ2HYz5XMjWdc77xOuf8o/+yssiEx/RlUaNvaJQHn+zigT1d7DzUO5Zzu661hrJIJChSMV6xJ+kczfUVLG+q5qnrl/CC86tY0VRNS2MlSefoH/LlP0+GhiOjCb+MpF/GaNwXiYgYNNVWsHKJr7CVqqTTUFPuK6O11LC2pSao0T9350k0YixvqmZ5U/WcrVNEROaeAuyFqudh+O1Vvom5590Gyy5XhULJy2g8yaOHern/iU7uf6KT7Xs6Odadb4+QudVWlXHBxmbe89LNbD2jmadvbM6rLKuIiMhCpAB7IWq7E25/qW8E/so7fa9Ssmgc7Rpk7/H+oMmlOL2Do5wcGqVvaJSRoFWBVKW2irIIleURokGO8FgFsKAh/3Cj/iNBe7OxeJKB4Ti7DveNFcFYvbSaZ5zVwtYzlrJmaY2vSJeqVBdUsKsoM4ygbVoDgvcuqFg3GmpGLBZPUlke5YyV9SVpl1RERKSUFGAvNId/Ane/wZenvuJXfigL2qGOQe55vJ17Huvgnsc6ONA+kHG6sXZxxyqTZe41MmJQUR6d1OboWHNY5VFqq8p403PWc+EZS7lwUzOrmmuKuYkiIiKnFAXYC8mer8D974TmrfCcm6GqpdQpEnxRi5u3H+HH9x1mbUsNl5+7nGdubsnaXNbx7iHu3NnO3bvauHtXB4c7fa+RTbXlPOOsFt76/I2cu65xrPJYfVChL72scCrnODaaIJ50YznaaqlBRESktBRgLwTOwc5PwY4Pwsqr4LIf+uIhUlJd/SN8e9t+bvzNkxzrHmblkipuf+QEX7n1ScqjxtYzlvLsc5Zx6ZZW2nqHuWun77Rjz7GTACypreCZm1t411WbeObmFjavbsirUX8zo6LMZtVsnYiIiBSe2sGe75yDHf8f7PxHOO1N8Mwb1fFLCTnneOxwH1/7zZP88HeHGI4lePY5y3j7lRt53nkriMWT/P6JTu54tI3bHz3BIwd6x+atrojyjLNauGxLK5duWcY5axvnRS9ZIiIiMjPqaGYhcknYfj088UU4411w4efV9N4MdffHaOsdZlVzNfXVuW9Q+odGOdzpuzw+2DHA/rYBDrYNcKB9gIMdgwzHElSVR3j1Jet4+5Ub2bymMeuyOvqGuffxTloaKnn6xmblNouIiCwi6mhmoUnG4d63wv5vwdnvh/M/pWb4ZuCJoye54ZYn+OHdBxke9ZUCG6rLWbW0mlXN/lVVHuVI1xCHOgY40jE0qce+2qoy1rfWsnFlPc89bwUbltfy4q2rWVpfOeX6WxqqeMmFq4uybSIiIjI/KcCejxIjcPcb4fCP4Ly/g3M+qOA6D845fvdYB1/65RP8esdxKssivOZZ67hkcyvHu4c42jXE4c5BjnYNsWNfD0OxOGuW+o5HLtjYzNqWWta21LBmaQ2nLaulua4C0/4XERGRaVKAPd/EB+COV8HxW+CCz8JZf1HqFC0YQ7EEP7//CF++ZQ8PH+ihub6C9758M9c+73RaGqpKnTwRERE5RSjAnm/ufSuc+DVc/DXY+CelTs2C8PiRPr69bR8//N1BegZG2biijn+69mm85pJ1VFdES508EREROcUowJ5PEjE48lPY9GcKrqeQyq3+9u37+P3uTsqjxosuWM2bL1/PJZtb1TqHiIiIlIwC7Pmk635IDMPy55U6JfOGc44TPcPsOtzH44f72HW4l8eO9LH7SB/Do0lOX17HR15/Lq991mm0NExd6VBERESk2BRgzydtd/hh66WlTUeJdffH+O3DJ/j1juPc8WgbnSdHxsYtb6rirNUNXPPc07ny/JVcsrlFFRBFRERkXlGAPZ+03Q6N556SXaDvOtTLrTuO8+sdx3lgTydJB0vrK7niKct52ulL2LymgbNWN0yraTwRERGRUlKAPV8k49B+N2x4S6lTMqd2HurlEz94mG2PtAHwlNOaeM9LN/P8p67g/A1LVJZaREREFhwF2PNF9x8g3g/LnlPqlMyJo12D/POPdvGDuw7QUF3Oh193Lq9+5lpWLKkuddJEREREZkUB9nyRKn+97LLSpqPITg6N8rmbd3PDLXtIJh3vfOEm/uIlZ7GkrqLUSRMREREpCAXY80Xb7VC/CapXljolBeec4+EDPfzs/iN8744DdJ4c4ZXPWMsHXr2Fda21pU6eiIiISEEpwJ4PXBLa7oR1ryl1SgrGOceOfd38fPtRfn7/EQ60DxCNGM85dxnve+UWzt+wpNRJFBERESkKBdjzQc/DMNoDy55d6pTMysBwnHsea2fbo23c8odjHOoYpCxqXLZlGX/xkrO46oKVNNepFRARERFZ3BRgzwdj5a8XVgXHZNLxyMEebn+kjW2PnOD+JzoZTTiqKqJcenYr73352bzgaStVvlpEREROKQqw54O226H2NKhdV+qUTNtQLMGbPnM39z7eAcA5axt5xws38Zxzl3HhGUupqoiWOIUiIiIipaEAu9Sc8znYq64udUqmLZF0vPs/7+e+3R387RvP4xUXr2FZU1WpkyUiIiIyLyjALrW+x2CkfcGUv3bO8dHvPcT/PHCUj7/pPK57wRmlTpKIiIjIvBIpdQJOeQus/PV//moPX731Sd7xwjMUXIuIiIhkoAC71Npu921f120sdUqm9NPfH+Zvv/8wL9m6mo++/imlTo6IiIjIvKQAu5RS5a9bnw1mpU5NTvc+3sGf37CdCzct5T/esZVIZH6nV0RERKRUFGCX0sA+GDoCy+d38ZDdR/v4k3+/h7UtNdz4nmeohRARERGRHFTJsZRO3O6HrfO3guPtj5zgPV95gPJohO++91nqKEZERERkCgqwS6n9DqhsgcYtpU7JJEOxBH9/0yN89ddPsmlVPf/5pxexrrW21MkSERERmfcUYJfSiduh9bJ5V/56x/5urv/P7ew5dpK3X7mRD772XKpVLERERERkWhRgl8rAIV8G+6z3lDolY+KJJJ+7eTef+ckuWhsq+cH7LuXZ5ywrdbJEREREFhQF2KUy1v71/Ch/PTgS502fuZv7dnfyiovX8A9vOZ+m2opSJ0tERERkwVGAXSrtd0B5IzSdV+qU4Jzjr7/+B37/RCefve4CXves00qdJBEREZEFS830lUrbHdB6KURKX7b5xt/s5Uf3HuL9r9yi4FpERERklhRgl8JoH/Q9Bi3PKHVKeGBPFx/93kO84PwV/MVLzip1ckREREQWPAXYpdDziB82PbWkyejoG+a6z9/LquYa/v069c4oIiIiUggKsEuhZ4cfLildgB1PJPnTL95Pd3+Mr1x/MY2q0CgiIiJSEKrkWArdD0F5E9SsLVkS/vG/d3LXrnY++/YLOPe0ppKlQ0RERGSxUQ52KfQ8BEvOK1kHM7988Cifu3k3b758Pa+7VJUaRURERApJAfZcc0kfYJeoeb5DHYO858sP8NQNTXziTaUtAy4iIiKyGCnAnmsD+yHeX5IA2znHh771R+LJJF/+s4upUvfnIiIiIgWnAHuudQcVHEvQgsgvHzzGrTuO875XbGFta+2cr19ERETkVKAAe671PAQYNJ0zp6sdGI7z4e/sYMvaRt525cY5XbeIiIjIqUQB9lzreQjqN0HZ3OYg//OPdnKse4h/vOZ8ysv0s4uIiIgUiyKtuda9Y87LXz9yoIev3Pokb37OBraesXRO1y0iIiJyqlGAPZdG+6H/yTntYCaZdPzfb/yBptoKPvjauS2WIiIiInIqUoA9l3pTXaTPXQ72t2/fx4N7u/nYG59Ck3prFBERESk6BdhzaawFkbkJsNt7h/nk/3uUZ53dyqufWbpeI0VEREROJQqw51LPQ1DeALVz03vix77/MEOxBJ96y/lYiXqNFBERETnVKMCeSz1BBcc5CHbvfbyD/77nENe/+EzOWFlf9PWJiIiIiKcAe644B90PzVkHM1/4xW6W1ldy/YvPmpP1iYiIiIinAHuuDByA+ElYUvzy13uP93PrjuNc89wNVKs7dBEREZE5pQB7rvTMXQXHr966h/Kocc0Vpxd9XSIiIiIykQLsudIddJHeeG5RV9M7EOP7dx3gFc9Yy7KmqqKuS0REREQmU4A9V3p2QN1GKK8r6mq+d8cBBkcSvP3KjUVdj4iIiIhkpgB7rvQ8VPQeHOOJJF/99ZM846wWzlu/pKjrEhEREZHMFGDPhfgAnNxT9PLXv/rDMQ53DnLdC5R7LSIiIlIqCrDnQs8jgCt6gP3lW/awtqWGFz5tVVHXIyIiIiLZKcCeCz0P+WERi4js2N/Nfbs7eduVG4lG1GujiIiISKkowJ4L3TugrL6oXaR/5ZYnqa0q442XrS/aOkRERERkagqw50LPQ76DGSvO7j7RM8RP7jvEGy47jYaa8qKsQ0RERESmRwF2sTnnA+wilr/+xm37iCcdb3u+KjeKiIiIlJoC7GIbPAijvUULsIdjCb7x27284PyVbFhe3Da2RURERGRqCrCLrTuo4NhUnAqOP77vEF0nY1z3gjOKsnwRERERyY8C7GLr2eGHTcXpIv2/7znEhuW1XLK5pSjLFxEREZH8KMAutp6Hgi7S6wu+6I6+Ye7e1c7LLlqDmZrmExEREZkPFGAXWxErON68/ShJBy+7aE1Rli8iIiIi+VOAXUzxITj5RNEC7J/df4SNK+o4e01DUZYvIiIiIvlTgF1MIx3gklBT+Bzm9t5h7nlMxUNERERE5hsF2MWUGPTDstqCL/oXD6SKh6wu+LJFREREZOZKGmCb2VVm9riZ7TGzD2QYv87MfmtmfzCzh8zsRaVI54zFUwF2TcEX/dPfH2bTqnrOWq3iISIiIiLzSckCbDOLAp8Hrga2AG80sy1pk30YuMk59zTgDcAX5jaVs5TKwY5WF3SxbT3D3PN4By+7cLWKh4iIiIjMM6XMwb4I2OOc2+uciwHfB16eNo0DUlm0jcDROUzf7CWG/DBa2Bzsm7cfwTl4qVoPEREREZl3ykq47tXAodDnw8DFadN8DLjFzP4cqAWePzdJK5AiFRH56f1HOGt1g4qHiIiIiMxD087BNrPClnOYnjcCNzrn1gAvAr5lZpPSbGbvMLPtZra9vb19zhOZVSrALmAO9vHuIe7b3aHKjSIiIiLzVD5FRI6Z2RfN7IICrfsIsDb0eU3wXdjbgJsAnHP3AFXApD7BnXM3OOe2Oue2tra2Fih5BTDWikjh7k1u3n4U5+AlFyrAFhEREZmP8gmw7wbeDvzezP5oZtebWdMs1n0/sMnMNphZBb4S40/TpjkIPA/AzM7GB9jzKIt6CkUog/3T+w9z9poGzlyl4iEiIiIi89G0A2zn3IuB04CPAHXAvwNHzew7ZnZFvit2zsWB64FfAbvwrYU8amYfN7OXBZO9F7jOzHYA3wOudc65fNdVMgUug32se4jf7+5U7rWIiIjIPJZXJUfn3FHg74G/D4LqtwGvAt5gZvuAr+HLTE+rtQ/n3C+AX6R995HQ+53As/JJ47xS4Gb6fn6/L0Hz0gvVeoiIiIjIfDXjZvqcc791zr0ZWAl8Bzgd+ASw38x+bGYXFSiNC1d8ECKVMLle5oz87PeH2bK2kU2r6guyPBEREREpvBlHfma21Mz+Cl82+83AAPB14MvAFcDvzOy6gqRyoUoMFax4yJHOQe7f08VLVTxEREREZF7LK8A27yoz+3/4Fj8+A4wAfwascs693Tn3bmAdsA34mwKnd2FJDBasguPPtwfFQ9Q8n4iIiMi8Nu0y2Gb2CeAafAcxA8A3gBuccw+kT+uc6zWzbwA3FiidC1N8sGA52Hc82samVfVsXKHiISIiIiLzWT6VHD8EPIAvZ/1d59zAFNM/CHx8pglbFBKDBavguOtQL8/cPI/a+BYRERGRjPIJsJ/unPvjdCd2zj0KPJp/khaR+FBBioh09Y9wrHuYc9Y2FiBRIiIiIlJM+bSDPSG4NrPqEnWfvnAkClNEZNehPgC2rFOALSIiIjLf5VvJcZmZfcHMjgL9QL+ZHQu+W16cJC5g8cJUcnz0YC8AW9YowBYRERGZ7/Kp5LgBuAvf7vXjwL3BqLOBdwEvN7PLnHN7C57KhSoxCGWzz+TfeaiHloZKljVVFSBRIiIiIlJM+ZTB/gywFHiVc+7H4RFm9kp8V+afxvfsKODbwS5ADvbOQ31sUflrERERkQUhnyIizwM+nx5cAzjnfgR8MZhGUgrQTF88kWT3EQXYIiIiIgtFPgG2A57IMX53MI2kFKCjmSeP9zMSTyrAFhEREVkg8gmwb8d3gZ7N5fjeGwXAuaCS4+zKYO885Cs4nqMWREREREQWhHwC7L8EnmFmnzGzZakvg5ZF/gW4OJhGAJIxwM26iMjOg72UR40zVqoHRxEREZGFIJ9Kjr8BqvBB9F+aWU/wfVMw7ABuM7PwPM45t3HWqVyIEoN+OMsiIjsP97JpVQMVZXm1qCgiIiIiJZJPgH0QlbGevngQYBcgB/vSLcumnlBERERE5oVpB9jOucuLmI7FJxVgz6IMdufJEY73DLNlbUOBEiUiIiIixaZyB8WSGPLDWeRg7woqOKoFEREREZGFI58iIgCY2Ubg5cDpwVd7gZ84554sZMIWvAKUwX5ULYiIiIiILDh5Bdhm9gngA0A0bdQ/mdknnXMfKVjKFroClMHeebCX1oZKWhrURbqIiIjIQjHtIiJm9lbgQ8B9wCuATcHrFcA9wIfM7NoipHFhSsy+DPbOw71sUe61iIiIyIKSTxnsd+OD68udcz91zj0ZvH6K74Dm98CfFyORC1KqDPYMi4iMxpPsPnKSc1T+WkRERGRBySfAPhv4vnMunj4i+O77wTQCsy4isvdEP7F4krMVYIuIiIgsKPkE2DGgLsf4+mAagVlXcnz0YFDBUQG2iIiIyIKST4B9P/BOM1uePiLoOv0d+CIkAqEc7JmVwd55yHeRvlFdpIuIiIgsKPm0IvIJfHfpu8zsq8DO4PtzgD/B52D/r8ImbwEbK4M98wD7THWRLiIiIrLg5NOT4x1m9irgc8B700YfBK5xzt1ZyMQtaPFBiJT71wzsPNTLs89RF+kiIiIiC01e7WA7535mZjcDFwAbgq/3Ag8655KFTtyClhiccfnrjr4RTvQMs2WNyl+LiIiILDTTCrDNrA7YAfyHc+7f8OWx7y9mwha8+OCMi4fsOhx0ka42sEVEREQWnGkV8HXO9QNLgf7iJmcRSQzNuIm+VAsiW9SCiIiIiMiCk08NunuBrcVKyKIziyIiuw71sqyxkpaGygInSkRERESKLZ8A+wPA68zsT8zMipWgRSM+OOMc7J2HepV7LSIiIrJA5VPJ8V+AbuArwD+Z2ZPAYNo0zjn3vEIlbkFLzKwM9mg8ye6jJ3nOuZOaGxcRERGRBSCfAPt0wOGb5ANQBJhLfAgqW/Ke7cnjJ30X6WsaipAoERERESm2fNrBXl/EdCw+iZkVERnrIl0tiIiIiIgsSNMug21m68wsa5kHM6s2s3WFSdYiEJ9ZJcedh3qpKIuwcYW6SBcRERFZiPKp5LgPeGWO8S8LphEIcrDzL4Ptu0ivp1xdpIuIiIgsSPlEcVO1HBLBl9EW8O1gzyAHe3/bgHKvRURERBawfLNJcwXQZwM9s0jL4jKDZvqccxzrGmJV88x6gBQRERGR0stZydHMrgGuCX31YTO7LsOkzcC5wI8KmLaFKzkKLp53DnbnyRgj8SSrlirAFhEREVmopmpFpAnYELx3QCuQHjU6fBfqXwM+VNDULVTxoHnwPHOwj3b5+ZSDLSIiIrJw5QywnXOfBT4LYGZJ4C+dc9+di4QtaIkhP8yzo5mjXX6+Vc0z6wFSREREREovn3aw1azFdCWCHOw8i4ikAuzVysEWERERWbAUNBfDjIuIDFEeNZbWVxYhUSIiIiIyF/IKsM3sDWZ2t5m1mVkiwyterIQuKPGZ52CvbK4mEpmqRUQRERERma+mXUTEzN4HfAroBO4NhpJJqgx2nh3NHO0aVPlrERERkQVu2gE28G7gPuB5zrmhIqVncZhFGewLNy0tQoJEREREZK7kU0RkBfBtBdfTMIMy2Mmk43i3OpkRERERWejyCbD34NvFlqnMIAe7o2+E0YRj1RIF2CIiIiILWT4BXGMqPAAAIABJREFU9meAt5lZXbESs2jMoB3ssU5m1IujiIiIyIKWTxnsBNAGPGZmXwP2Bd9N4Jz7ZoHStnDNoIjIEXUyIyIiIrIo5BNg3xh6/+Es0zhAAfYMioiM9+KoHGwRERGRhSyfAPuKoqVisYkPgkUhUj7tWY52DVFVHqG5rqKICRMRERGRYsunq/Tbi5mQRSUx5Mtf2/Q7jDkWdDJjecwjIiIiIvPPjLpKN7NKM1ttZspuzSQ+OINu0tXJjIiIiMhikG9X6U83s9uAk8BB4NLg+2Vm9hsze34R0rjwJAbz7mTmSJfawBYRERFZDKYdYJvZ+cCdwEbSKjI659qAauCagqZuocozBzuRdJzoGVaALSIiIrII5JOD/XHgKHAO8AEgvbDwb4CLCpSuhS1VBnuaTvQMkUg6BdgiIiIii0A+AfZlwJedc/345vjSHQRWFSRVC12eRUSOdQ0DqBdHERERkUUgnwC7CujNMb5hlmlZPPIsIjLei6MqOYqIiIgsdPkE2E8CF+QY/1xg5+ySs0jkmYN9RJ3MiIiIiCwa+QTY3wX+d1pLIQ7AzN4LXAV8q4BpW7ji+ZXBPto1RE1llMaa6XdMIyIiIiLzUz49OX4auBL4FfAYPrj+VzNrBVYAtwJfKHgKF6JE/kVEVjXXqJMZERERkUVg2jnYzrkYPsD+a2AIGAbOBDqA9wMvcc4li5HIBSeebyVHtYEtIiIisljkk4ONcy4O/GvwkmzyzsEe4vKnqI6oiIiIyGIwo67S05lZZSGWsygkE5CMTbsM9mg8yYledTIjIiIisljk05Pj1Wb2sbTv/szM+oABM/uumamWXsK3CDLdHOzjPcM4pxZERERERBaLfHKw3wdsTn0ws7OBz+J7d7wVeD3w7oKmbiFK+Datp1sG+1h3qok+tYEtIiIishjkE2CfDWwPfX49vrLjRc65q4EfANcUMG0LUzwIsKeZg320M+hkRjnYIiIiIotCPgH2EnyLISnPB25zzvUFn7cBGwqUroUrVURkmmWwj6qTGREREZFFJZ8AuwM4DcDM6oELgTtD48uBaOGStkDlWUTkaNcQ9dVl1Fer+LqIiIjIYpBPM333AO8ys0eBq4N5/yc0/gzgWAHTtjDlWUTkSNDJjIiIiIgsDvkE2B8FfgvcFHz+hnNuJ4D5LghfGYw/tcXzr+So4iEiIiIii8e0A2zn3M6g5ZBnAb3OuTtCo5vwnc9sK2zyFqCxZvqmWQa7c4hz1jYVMUEiIiIiMpfy7cmxC/hZhu+78U32SR5lsEdGE7T3jbB6qXKwRURERBaLfDqaWRrkYIe/22Bm/2Fm3zGzFxY+eQtQHmWwj3cPA2pBRERERGQxyScH+7PAmcBFAGZWh29FZFUw/vVm9ty0oiOnnjxysI90pdrAViVHERERkcUin2b6ngn8IvT59fjg+kXBcBfw/sIlbYHKox3sY2oDW0RERGTRySfAXg4cCn2+GtjunPulc+44cCPwtHxWbmZXmdnjZrbHzD6QZZrXmdlOM3vUzL6bz/JLYqwVkaopJ1UnMyIiIiKLTz5FREaBcCT4HHxQndIDLJ3uwswsCnweuBI4DNxvZj9NNf0XTLMJ+P+AZznnus1sWR7pLY3EoC8eYjblpEe7hmiqLaemMq+6piIiIiIyj+WTg70beLV5LwOagd+Exq8FuvJY3kXAHufcXudcDPg+8PK0aa4DPh+0UoJzri2P5ZdGfHDancwcVSczIiIiIotOPgH25/G51t3AD4G9TAywLwMezmN5q5lY5ORw8F3YmcCZZna3md1rZldlWpCZvcPMtpvZ9vb29jySUASJoWmVvwafg63iISIiIiKLSz4dzXzTzBzwCqAX+KRzbhR8E374zma+UIT0bQIuB9YAd5jZU5xzPWlpuwG4AWDr1q2uwGnITz452N1DPH1jc5ETJCIiIiJzKd+OZr4FfCvD953ABXmu+wi+WEnKmuC7sMPAfUEgv8/MduMD7vvzXNfcSZXBnsJQLEHXyRgrlygHW0RERGQxyaeIyJig05mtwWvaFRvT3A9sCjqrqQDeAPw0bZof43OvMbMWfJGRvTNc39yYZg72WBN96sVRREREZFHJK8A2s6ea2e1AG3Bf8Gozs21mdl4+y3LOxYHrgV/h29C+yTn3qJl9PKhESTCu08x2Ar8F3hfkls9f0yyDfTToZGa1KjmKiIiILCrTLiJiZucCdwFVwE+AR4NR5wAvBe40s0ucc49mWcQkzrlfMLHzGpxzHwm9d8D/+f/bu/PwqKps///vRSAJkDCjooIgiIqogCiOqDQGhxZthYsKIsrggIpDowJeG5XrwM/WbkVaxICA0IojyEVxRLu/XgcUtEFRUHBEZMxAEpJK9u+POomZUwVVFarO5/U89aTqnF1nr12VKhY76+zj3eJDcR6ktKmz2S87tAa2iIiISCIKpwb7HoJrYZ/inPui/A4v+X7fa3Nx5MKLQyGWiPyyLZhgqwZbREREJLGEUyLSl+Ca1F9U3uGcW01wBZHTIxVY3ArxJMdftufTKj2Z1OSkGAQlIiIiIrESToLdFPi1lv2bvDb+FkYNtspDRERERBJPOAn2d8Afa9n/R/b1FT5iIdQSke35OsFRREREJAGFk2DPBQaY2QIzO8rMkrxbdzObD2QAT0clynjhSrwZ7LoT5193FKj+WkRERCQBhXOS40NAL4LrVQ8BSrztDQADFgJ/jWh08aa4IPizjhnskhLHzrxCWqUnxyAoEREREYmlcC6VXgwMMbOnCF4uvZO36zvgFefcW1GIL74UB1cGqasGO7cggHPQrEmjGAQlIiIiIrEUUoJtZk2BWwletnwZ8GZUo4pXgeDFY+qawc7aVQhAcyXYIiIiIgknpBps59wuYCLQPrrhxLliL8GuowY7K68I0Ay2iIiISCIK5yTHb4EDohVIQghxBjvbS7BbNFENtoiIiEiiCSfBng6MNrPW0Qom7oVYg60ZbBEREZHEFc4qIjnAduBrM5sDrAPyKjdyzs2NUGzxJ8QSkdIZbNVgi4iIiCSecBLsp8vdv7mGNo7getn+FOpJjprBFhEREUlY4STYZ0YtikQRCHUGuxAzSG+sBFtEREQk0YSzDvZ70QwkIZTWYDesvQZ7Z14R6amNaNDAYhCUiIiIiMRSnSc5mtkRZnbE3rbxhTBqsFUeIiIiIpKYak2wzaw3sAYYUMdxBgCrzeyYSAUWl8JYpk8nOIqIiIgkprpmsEcDG4DH6mj3GMFLpl8TiaDiVtkMdt3L9GkGW0RERCQx1ZVgnwG86Jwrqa2Rt/9F/H4iZHE+NEgBq/1lzc4ronlTJdgiIiIiiaiuBLs98E2Ix1oPHLJ34cS5QF6d5SEAWXmFKhERERERSVB1JdglhL7SSEOvvX8V59V5giOUnuSoy6SLiIiIJKK6EuwfgR4hHqsH8NPehRPnQpjBLi5x5OQHNIMtIiIikqDqSrDfBS41s/1ra+TtvxR4O1KBxaXi/DpPcMzWVRxFREREElpdCfYjQGNgmZkdXl0DM+sKvAakAn+LbHhxJlB3iUhpgq0ZbBEREZHEVGt9tXNunZldB8wA1pjZv4FVQDaQDvQETgEMGO2cWxflePdtxXWXiGTlFQKawRYRERFJVHWewOicyzSz74EHgb7erbyVwO3OubeiEF98CeRB4xa1NlGJiIiIiEhiC2mFEC95Ps7MOgLdgWYEZ7FXO+c2Riu4uFOcDw3rvsgMQIumWkVEREREJBGFugQfAF4yvTEqkSSCEJbp0wy2iIiISGKr6yRHCUcIy/Rl6SRHERERkYSmBDuSQpjBztpVSAODpilh/fFAREREROKEEuxIcc6rwa67RKRZk0Y0aGAxCkxEREREYkkJdqSUFIIrqfNCM1legi0iIiIiiUkJdqQU5wV/hnCSoxJsERERkcSlBDtSAl6CXddJjvlFNG+iJfpEREREElWNZ9qZWYc9OaBz7oc9DyeOFecHf4Ywg33o/mkxCEhERERE6kNtS1lsBNweHDNpz0KJc2Uz2LXXYGfnFWmJPhEREZEEVluCfQ97lmD7U4g12DrJUURERCSx1ZhgO+cmxzCO+BdCDXZRoIRdBQEl2CIiIiIJTCc5RkoINdjZ+aVXcdRJjiIiIiKJao8uJ2hmaUALqknQ/XuSY2mJSM012NneZdI1gy0iIiKSuMJKsM3sEuBO4Mhamvn8JMeaZ7Cz8kpnsJVgi4iIiCSqkEtEzOxCYAHBpHwGYMA/geeBIuBTgidG+lMIJzlqBltEREQk8YVTg/1n4CugB3CXt22Wc+4SoDdwOLAqsuHFkYBXg13LDHa2ZrBFREREEl44CfYxwBznXAFQ4m1LAnDOrQaeBCZENrw4EkINdlZeIQDNmyrBFhEREUlU4STYScA27743XUvzcvu/BrpHIqi4FMiDBo2gQc1l7Vm7tIqIiIiISKILJ8H+CTgEwDmXD/wGHFdu/+HArsiFFmeK80K6yExSA6NJij/PAxURERHxg3BWEfkA6M/v9deLgZvMLJ9goj4WeDWy4cWR4vxa668hWIPdrEkjzCxGQYmIiIhIrIWTYE8H/mRmjb0Z7EnACcBkb/8agidC+lMgr9b6awjOYOsERxEREZHEFnKC7Zz7BPik3OMtQA8zOwYoBr5yzpXU9PyEF0KJSHZeoZboExEREUlwe3Qlx/Kcc19EIpC4F8gLqUREM9giIiIiiS2cC830N7P7a9l/v5mdGZmw4lBxfkgnOWoGW0RERCSxhbOKyG1Al1r2dwJu37tw4lgINdjBGWwt0SciIiKSyMJJsI8FPqxl/0deG38qrrtEZKdKREREREQSXjgJdnNqX+c6H2i5d+HEsUDtJznuLiqmoLBYJSIiIiIiCS6cBPtnKl5YprLjgF/3Lpw4Vsc62Dn5pVdxVIItIiIiksjCSbD/F7jCzPpX3mFmfwCuAJZGKrC4U1x7DXZWXjDB1gy2iIiISGILZ5m+/wEuBpaZ2WvAKm97D+AcgrPX90Y2vDhSxzJ92XmawRYRERHxg3AuNLPZzE4G/kEwoT63dBfwGnC9c25T5EOMAyVF4AK11mCXzWA31SoiIiIiIoksrAvNOOe+B841s5b8vmTfeufcjohHFk+K84M/NYMtIiIi4nt7dCVHL6H+pM6GfhHIC/6spQZ7565CQAm2iIiISKIL5yRHqUlxaYJd9wy2TnIUERERSWw1zmCbWQlQAjRxzhV6j10dx3POuT2aFY9rpTPYtZSIZOUV0SjJaJycFKOgRERERKQ+1JYMzyWYUBdXeiyVldZg1zGD3axJMmYWo6BEREREpD7UmGA750bU9ljKKZvBrrkGO1uXSRcRERHxhZDKOcysKXAr8JFzbll0Q4pDbU+GizZDo+Y1NsnKK1L9tYiIiIgPhHSSo3NuFzARaB/dcOJUg0aQuh8kpdTYJFsJtoiIiIgvhLOKyLfAAdEKJNFlqURERERExBfCSbCnA6PNrHW0gklkWbsKNYMtIiIi4gPhLKmXA2wHvjazOcA6IK9yI+fc3AjFllCy84poocuki4iIiCS8cBLsp8vdv7mGNo7gcn5STkFhMbsDJZrBFhEREfGBcBLsM6MWRYIrvYqjarBFREREEl/ICbZz7r1oBpLIsnSZdBERERHfCOckxwrMrI2ZtYlkMIkqWwm2iIiIiG+ElWCb2YFmNsfMdgKbgc1mtsPMnjazg6ITYvzLyisEVCIiIiIi4gchl4iYWQfgQ4JrYa8C1ni7ugHDgbPM7ETn3I8RjzLOqURERERExD/COcnxXqAl8Efn3NLyO8zsHOAlr82IiEWXIHSSo4iIiIh/hFMikgFMr5xcAzjnXgP+AZwdqcASSVZZgq11sEVEREQSXTgJdkuCF5epyTqgRTidm9nZZva1ma03sztqaXexmTkz6x3O8fcV2XlFpDRsQGpyUn2HIiIiIiJRFk6C/RNwRi37+3ptQmJmScDjwDkE67gvNbNu1bRLB8YBH4UR6z4lK0+XSRcRERHxi3AS7OeBwWZ2v5k1L91oZs3M7D7gv4DnwjjeCcB659x3zrlC4Fnggmra3Qs8CBSEcex9SnZekRJsEREREZ8IJ8G+F/g/4HZgq5l9b2bfA9uAO4APgClhHO8goPyKIz9528qYWS+gvXPuf8M47j4nK69IJziKiIiI+ETICbZzLo9gicjVwBvALu+2DBgDnOmcy49UYGbWAHgYuDWEtmPMbIWZrdiyZUukQogYzWCLiIiI+Ec4y/ThnAsAM73b3voZaF/u8cHetlLpQHdguZlBcP3txWY20Dm3olJcTwJPAvTu3dtFILaIytpVRIc2Tes7DBERERGJgT2+VHoEfAIcZmadzCwZuARYXLrTOZflnGvjnOvonOtI8CI3VZLreJClGWwRERER3wjnSo531dHEAfnAD8By59xvtTZ2LmBm1xMsMUkCZjnn1pjZPcAK59zi2p4fL5xzZOcV0rypEmwRERERPwinRGQywSQawCrtq7y9yMwecs5Nqu2A3kVrllbaVm0i75w7I4xY9xn5hcUUFTud5CgiIiLiE+GUiHQHPiO4ksgQoId3u4Rg+cYK4ERgsHf/DjO7OqLRxqHSy6SrRERERETEH8JJsEcTXIv6dOfc8865L7zbQuB0oAi4xDn3ovf4PwRXHPE1XSZdRERExF/CSbAvARY654or7/BWF1notSn/+PBIBBnPNIMtIiIi4i/hJNjNvVtt+1uUe7yV32uzfSu7bAZbCbaIiIiIH4STYH8OXGdmh1TeYWYdgeuAVeU2Hw5s2pvgEsHOXYWAZrBFRERE/CKcVUTuILik3ldm9grwjbf9cOACgsn6pQBmlgIMBZZELtT4pBlsEREREX8JOcF2zr1nZv0JXr78kkq7VwB/ds6977Xd7c10F0Us0jiVpRpsEREREV8J91Lp/wZOMLP9gE7e5o3Ouc3VtN0dgfjiXnZeEanJSaQ0SqrvUEREREQkBsJKsEt5V2ms9UqNEpSVV6TyEBEREREfCeckR8wsycyGm9kzZvammfX0trf0th8UnTDjV3ZekcpDRERERHwk5BlsM2sCvAGcDOwCmgAtvd3ZwAPALODOCMcY17LzNYMtIiIi4ifhzGBPBnoDfwIOBax0h3fxmZeAAZEMLhHs3FWoGWwRERERHwknwR4MPOmcWwSUVLN/PdAxEkElkmzVYIuIiIj4SjgJ9oEELzZTkzwgfe/CSTxZqsEWERER8ZVwEuxtQG0nMR4F/LJ34SQW55xOchQRERHxmXAS7LeBK72THSsws07AVcDrkQosEeTtLqa4xNGiSXJ9hyIiIiIiMRJOgn03wVVDPgGuBRxwtpndD3wG7Abuj3iEcSwrrxCAtMZ7tNy4iIiIiMShkBNs59x64A9AALiH4CoifwZuB34E/uCc+zEaQcar3IIAAM0aq0RERERExC/CvVT6p8CxZtYdOJJgkr3OObcyGsHFu9z8YIKdlqoZbBERERG/COdCM32Br5xzW5xzq4HVlfa3Abo5596PcIxxq3QGO00z2CIiIiK+EU4N9rvAWbXs/4PXRjw5+UWAZrBFRERE/CScBNvq2J9E9Reg8a3SEpF0zWCLiIiI+EY4CTYEVw6pycnA1r2IJeH8XiKiGWwRERERv6g18zOzccC4cpv+Zmb/U03TlkAzYFYEY4t7KhERERER8Z+6Mr+dwPfe/Y4Er+a4uVIbR/CExw+BRyIZXLzbVRAguWEDUhol1XcoIiIiIhIjtSbYzrk5wBwAM9sA3OGcWxyLwBJBTkERTTV7LSIiIuIrIWd/zrlO0QwkEeXkB0hX/bWIiIiIr4R7kqOEYVdBgPRUrSAiIiIi4idhJdhmdoqZLTGzLWYWMLPiSrdAtAKNRzn5RTTVDLaIiIiIr4ScYHtXcnwX6AN85D33XeATgmtkrwbmRSHGuJVbECBdNdgiIiIivhLODPYkYBPQDRjhbbvPOXcicDbQCXgqotHFudz8AGkqERERERHxlXAS7BOAp5xzW/j9io0NAJxzbxCcvb43suHFt5z8Il1kRkRERMRnwkmwU4Cfvfu7vZ/p5favAo6LRFCJIrcgoMuki4iIiPhMOAn2JuBgAOfcLoIXoelebv/BgE5y9ASKS8gvLNZVHEVERER8Jpzs7xPglHKP3wBuNrPvCSbq1xM8+VEILtEHqERERERExGfCmcHOBLaaWWPv8UQgH3gamEWwbOS2iEYXx3K8BFvrYIuIiIj4SzhXcnwTeLPc4+/MrCvwB6AY+LdzLivyIcYnzWCLiIiI+NNeZX9eLfbiCMWSUHLyiwBUgy0iIiLiM7WWiJhZkpk9YGbX1NHuWjO7z8wssuHFr5z80hlslYiIiIiI+EldNdjDgPEET3CszcfA7cClkQgqEewqq8HWDLaIiIiIn9SVYP8X8JZz7tPaGnn7l6EEu0xZiYhmsEVERER8pa4E+zjgrRCP9S7Qe+/CSRy5pSc5agZbRERExFfqSrBbAb+FeKwtXntBJzmKiIiI+FVdCXYO0CbEY7UGcvcunMSRmx8gtVEDGjUMZ6lxEREREYl3dWV/a4CMEI91ltdeCJaIqP5aRERExH/qSrBfAvqb2QW1NTKzgQQT7BcjFVi8yy0oIl0XmRERERHxnboS7BnAemChmf2PmXUsv9PMOprZFGAh8I3XXgiWiDRV/bWIiIiI79SaATrn8s3sPGAJMAG4w8yyCdZmpwPNAAO+Bv7onCuIcrxxI6cgQHqqSkRERERE/KbOM/Ccc+uBHsA44N9AMXCA9/Nf3vZezrlvoxhn3MnNL9IKIiIiIiI+FFIG6M1MP+bdJAQ6yVFERETEn7SGXJTk5Ad0kqOIiIiIDynBjpJdBSoREREREfEjJdhRUBgooaCoRCUiIiIiIj6kBDsKcgt0mXQRERERv1KCHQW78gMAqsEWERER8SEl2FGQUxBMsNO0DraIiIiI7yjBjoLc0gRbM9giIiIivqMEOwpy8oM12LqSo4iIiIj/KMGOgtx8zWCLiIiI+JUS7CjQKiIiIiIi/qUEOwpyylYRUYmIiIiIiN8owY6CXd5Jjk01gy0iIiLiO0qwoyAnv4gmKUkkNbD6DkVEREREYkwJdhTkFgRUfy0iIiLiU0qwoyA3P6D6axERERGfUoIdBTkFRVqiT0RERMSnlGBHQbBERDPYIiIiIn6kBDsKcvNVgy0iIiLiV0qwoyC3oEgJtoiIiIhPKcGOghyd5CgiIiLiW0qwoyA3Xyc5ioiIiPiVEuwI211UTFGxU4mIiIiIiE8pwY6wnPzgZdJVIiIiIiLiT0qwI2xXQTDB1gy2iIiIiD8pwY6wnPwiANVgi4iIiPiUEuwIyylQiYiIiIiInynBjrBdpTPYKhERERER8aV6TbDN7Gwz+9rM1pvZHdXsv8XMvjSzL8zsbTM7pD7iDEdOWQ22ZrBFRERE/KjeEmwzSwIeB84BugGXmlm3Ss1WAr2dc8cALwBTYxtl+HJLE2zVYIuIiIj4Un3OYJ8ArHfOfeecKwSeBS4o38A5965zLs97+CFwcIxjDFuuVyKSrhIREREREV+qzwT7IODHco9/8rbVZCTwWlQjioCc/ABm0CRFCbaIiIiIH8VFFmhmw4DewOk17B8DjAHo0KFDDCOrKrcgQNOUhjRoYPUah4iIiIjUj/qcwf4ZaF/u8cHetgrMrD8wCRjonNtd3YGcc08653o753q3bds2KsGGKje/iHTVX4uIiIj4Vn1mgp8Ah5lZJ4KJ9SXAZeUbmFlPYAZwtnPut9iHGL7cggBNtYKIiIjvZWVlsXXrVgoLC+s7FBEJUVJSEunp6bRq1YqUlJQ9Pk69JdjOuYCZXQ8sA5KAWc65NWZ2D7DCObcY+P+ANOB5MwP4wTk3sL5iDkWOZrBFRHyvoKCAzZs3c/DBB9O4cWO8f8NEZB/mnKOoqIjs7Gx++OEHOnTosMdJdr1mgs65pcDSStvuKne/f8yD2ku5BQFdxVFExOe2bNlC27ZtadKkSX2HIiIhMjOSk5Np06YNANu3b6ddu3Z7dCxdyTHCgiUimsEWEfGzgoIC0tLS6jsMEdlDzZo1IycnZ4+frwQ7wnLyi7QGtoiIzwUCARo21L8FIvGqUaNGFBcX7/HzlWBH2K6CAGlKsEVEfE911yLxa28/v0qwI8g5R05+gDTVYIuIiIj4lhLsCMovLKa4xGkVEREREREfU4IdQbsKAgAqEREREYmwjRs3YmZMnjx5j48xYsQIle5ITCjBjqCcfC/BVomIiIgkODML+bZx48b6Dnef1adPH8yMkSNH1ncoEkGaao2g3IIiQDPYIiKS+ObNm1fh8b/+9S+efPJJxowZw2mnnVZhX9u2bfe6v0MOOYT8/Py9Wp1l5syZPPHEE3sdS6SsXr2ajz/+mM6dO7Nw4UIeffRRmjZtWt9hSQQoE4ygXG8GWxeaERGRRDds2LAKjwOBAE8++SQnnXRSlX2V5eTkkJ6eHlZ/ZkZqamrYcZbXqFEjGjXad/6NzszMJD09nWeeeYaTTjqJhQsXcuWVV9Z3WHXak/fPb1QiEkG5pTXYOslRREQEgI4dO3LGGWewcuVKBgwYQPPmzTnmmGOAYKJ255130qdPH9q0aUNKSgpdunThjjvuIC8vr8JxqqvBLr9tyZIlHH/88aSmptKuXTvGjx9PIBCocIzqarBLt2VlZXHttdey3377kZqayimnnMJHH31UZTzbtm3jqquuonXr1qSlpdGvXz9WrlzJGWecQceOHUN+XQoLC3nmmWcYNGgQJ554Ij179iQzM7PG9i+++CJnnHEGLVq0oEmTJhx++OHceOONFBYWlrVxzjFz5kz69OlDWloaaWlpHH300dx1V9lFspk8eXKNZTul71V5ZsaIESN4++23OfXUU0lLS+P8888H4JdffuHWW2+lR48etGzZktTUVLp168aDDz6PsXdSAAAgAElEQVRY7RrShYWFTJ06lR49etCkSROaN29O7969mTZtGgCPPPIIZsabb75Z5bm7d++mdevW9OvXr9bXdV+hTDCCcvJVIiIiIlLZDz/8QL9+/Rg8eDAXX3wxubm5APz888889dRTXHzxxVx22WU0bNiQ9957j6lTp7Jy5UqWLVsW0vGXLl3K9OnTueaaa7jqqqtYtGgRDz30EC1btmTixIkhHWPAgAG0bduWu+66i23btvHwww9z3nnnsWHDhrLZ2t27d9O/f39WrVrFiBEjOOGEE/jiiy/o378/rVq1Cus1WbRoEVu3buWKK64Agon+uHHj+Prrrzn88MMrtJ00aRL33Xcf3bp14+abb6Zdu3Z8++23vPjii9xzzz0kJycDcPnllzN//nz69OnDpEmTaNGiBWvXruWFF17gnnvuCSu+8lasWMGLL77I6NGjy+IF+OKLL3jppZf405/+ROfOnSkqKuL111/njjvu4LvvvmPGjBllbQsLCxkwYADLly8nIyODYcOGkZqayn/+8x9eeuklrr/+eoYPH86ECROYNWsWZ511VoUYXn75ZbZv386oUaP2eByxpEwwgkpnsNNT950/P4mIyL7jv+d/zpofsuo7jAqO6tCce4ceG9U+NmzYwMyZM6skR4ceeig//vhjhbKNsWPH8t///d9MmTKFjz/+mBNOOKHO469Zs4Y1a9aUzSBfc801HH300Tz22GMhJ9i9evVi+vTpZY+7devGf/3Xf7FgwQKuvvpqIFjSsWrVKqZMmcKkSZPK2h599NGMHTuWQw45JKS+AGbNmkXHjh3p27cvAJdddhl//vOfmTVrFg8++GBZu48//pj77ruPM888k6VLl1Yok3nggQfK7i9cuJD58+czbNgw5syZQ4MGvxcplJSUhBxXddasWcObb75J//79K2w//fTT+e677yr8VeCmm27i8ssv56mnnmLy5Mm0a9cOgL/97W8sX76cCRMmcN9991U4Tml8rVu35qKLLuKll15i+/btFf7TkpmZScuWLbnooov2aiyxohKRCFKJiIiISFWtWrWqtrY4OTm5LLkOBALs2LGDrVu3liVy1ZVoVOfCCy+sUJ5hZpx55pn8+uuvZbPldbn55psrPC4tRVi3bl3ZtldffZWkpCTGjRtXoe2oUaNo3rx5SP0A/Pjjj7zxxhsMHz68LDlt06YN5513HnPnzq1Q2jJ//nwA7r///io16KWrtJRv99BDD1VIroEqj8N17LHHVkmuARo3blzWf2FhIdu3b2fr1q0MGDCAkpISVqxYUWEcLVu2rFCuUl18Y8aMYffu3WXjgWAp0Ntvv83QoUP3ug4/VpQJRlBOfhENDBonJ9V3KCIisg+K9kzxvqpz584kJVX/b+P06dN54oknWLNmTZWZ1h07doR0/EMPPbTKttatWwPBmum0tLSwj1H++aU2bNjAgQceWOV4ycnJdOrUKeR4n376aUpKSjjllFNYv3592fZ+/frxyiuvsHTpUgYOHAgEE3wz49hja//dWbduHe3atWP//fcPKYZwdO3atdrtgUCABx54gLlz57J+/XqccxX2l3891q1bR48ePepMkM844wy6du1KZmYmN9xwAwCzZ8/GORc35SGgBDuicgsCpDdupEXsRUREymnSpEm12x9++GFuvfVWMjIyuPHGGznwwANJTk7m559/ZsSIESGXNtSUvANVkr5wjxHq80PlnGP27NlAsO67OrNmzSpLsKHiTPXequ04lU8KLVXT+3fLLbfw2GOPMWTIECZNmsR+++1Ho0aN+Oyzz7j99tv3uDRl9OjRjB8/nk8//ZSePXvy9NNP07t37zr/k7EvUYIdQbn5RTTVCY4iIiIhmTdvHh07duS1116rUCbw+uuv12NUNevYsSNvvfUWubm5FWaxi4qK2LBhAy1atKjzGO+++y4bNmzgpptu4pRTTqmy/5///CeLFy9m8+bN7L///nTt2pXXXnuNzz//vNZ69K5du7Jo0aKy59WktK55+/btFcpqCgoK2LRpE126dKlzDKXmzZtH3759efbZZytsLz8rXz6+tWvXsnv3blJSUmo97ogRI5g0aRKZmZlccMEF/PDDD0yYMCHkuPYFqsGOoJz8gNbAFhERCVFSUhJmVmGWuLTsYF90/vnnU1xczN///vcK22fOnElWVmgnr2ZmZpKUlMTEiRMZNGhQlduNN95IIBBg7ty5QPDkR4CJEydWWJKvVOlrN3ToUABuu+22KjPH5V/f0nKPt956q0KbRx55JOwZ56SkpCoz/Lt27eKRRx6p0nbo0KHs2LGDKVOm1DiGUm3atOHCCy9kwYIFTJs2jSZNmpS9DvFC060RlFsQ0BJ9IiIiIRo0aBATJkzgnHPO4aKLLiI7O5sFCxbsUxeDKW/UqFHMmDGDO++8k/Xr15ct07dw4UK6dOlSY4lFqZ07d/LSSy9x2mmn1Xh1y9NOO4399tuPWbNmMX78eE444QRuv/12HnzwQXr16sWQIUM44IAD2LBhAy+88AIff/wxLVq0YPDgwQwZMoS5c+eybt06Bg4cSMuWLfnmm29YtmwZq1evBqB///4cfvjhZcsRdurUiX//+998+OGHtGnTJqzXY9CgQcyYMYMhQ4bQv39/Nm/ezKxZs8rq18sbN24cr776KlOmTOGTTz4hIyOD1NRU1qxZw9dff10l4R8zZgwLFy5kyZIlXHHFFTRr1iys2OqbssEIys0vIr3JvvmlICIisq8ZP348zjkyMzMZN24cBxxwAEOGDOHKK6+kW7du9R1eFSkpKbz99tuMHz+eRYsWsXDhQvr06cPbb7/NqFGjqlwcp7L58+dTUFBQ61JzDRo04MILL+TJJ5/kgw8+4OSTT+aBBx7g2GOPZdq0aUydOpWSkhLat2/PueeeW6E+esGCBZx22mlkZmZyzz33kJSURKdOnRg8eHBZm6SkJBYvXsyNN97IY489RnJyMhkZGbz33nvVlqzU5uGHHyY9PZ2FCxeyaNEi2rdvz5gxYzj++OOrrDqSnJzMG2+8wV//+lcWLFjAxIkTSU1N5bDDDqt2hZl+/frRpUsX1q9fz8iRI8OKa19gkS7er2+9e/d25ZeFiaXTJ75J1wObMfP6PvXSv4iI7Bu++uorjjzyyPoOQ2KkuLiYNm3a0KdPn322fjweHXXUURQXF7N27dp66T+Uz7GZfeqc6115u2qwIyi3IKA1sEVERBJYfn5+lW1PPPEEO3furHL1Qdlz77zzDl9++SWjR4+u71D2iLLBCMrJL1INtoiISAIbPXo0BQUFnHzyyaSkpPB///d/LFiwgC5dujBmzJj6Di/uvfPOO3z77bfcf//9tG3bVgm23znndJKjiIhIgsvIyODxxx/n3nvvJTc3l/33359Ro0Zx7733kp6eXt/hxb177rmHf//733Tr1o05c+bE3cmNpZQNRkje7mKcQ8v0iYiIJLDhw4czfPjw+g4jYS1fvry+Q4gI1WBHSG5BEYBqsEVERER8Tgl2hOTkB9e+TEvVDLaIiIiInynBjpDc/OAMdrpmsEVERER8TQl2hOQWBGewm+okRxERERFfU4IdIaUlIjrJUURERMTflGBHSOkMtkpERERERPxNCXaElNZgax1sEREREX9Tgh0hOQVaRURERERElGBHTG5BEY2SjJRGeklFRET2xsaNGzEzJk+eXGG7mTFixIiQjjF58mTMjI0bN0Y8vqeffhozS5iLokjkKRuMkNz8AGmpjTCz+g5FREQk6gYPHoyZsWrVqhrbOOfo1KkTLVq0ID8/P4bR7b3ly5czefJkdu7cWd+h1Km4uJiDDjoIM+Pee++t73AEJdgRk1sQ0FUcRUTEN0aOHAnA7Nmza2zz7rvvsnHjRi655BIaN268133m5+czc+bMvT5OKJYvX87dd99dbYJ9+eWXk5+fT9++fWMSS11ee+01fvnlFzp37szTTz+Nc66+Q/I9JdgRkpNfpBMcRUTENzIyMmjfvj3z58+nsLCw2jalyXdpMr63UlNTadSo/s91SkpKIjU1lQYN9o00KjMzk86dO/Pwww/z3XffxU3pSk5OTn2HEDX7xm9GAthVECBNa2CLiIhPNGjQgBEjRrBt2zYWL15cZX92djYvvvgi3bt35/jjjycnJ4c777yTPn360KZNG1JSUujSpQt33HEHeXl5IfVZXQ12SUkJ999/P506dSI1NZXu3bszf/78ap+/du1arrvuOo466ijS09Np0qQJxx13HE899VSFdiNGjODuu+8GoFOnTphZhZrwmmqwt27dytixY2nfvj3Jycm0b9+esWPHsm3btgrtSp//zjvv8NBDD9G5c2dSUlLo2rUrc+bMCem1KLV582aWLFnC8OHDOffcc9lvv/3IzMystq1zjpkzZ9KnTx/S0tJIS0vj6KOP5q677qrQrrCwkKlTp9KjRw+aNGlC8+bN6d27N9OmTavwGtVUFlv5fSpfU//cc89x3HHH0bhxY2644QYg9PelVHZ2NpMmTeLII48kNTWV1q1bc+qpp/Lss88CMG7cOMyMdevWVXnupk2baNiwIVdddVXNL2oEaMo1QnLyA7RKT67vMERERGLmyiuvZMqUKcyePZtBgwZV2Pfss8+Sn59fNnv9888/89RTT3HxxRdz2WWX0bBhQ9577z2mTp3KypUrWbZs2R7FcMstt/D3v/+dvn37cvPNN/Pbb78xduxYDj300Cptly9fzvvvv88f//hHOnXqxK5du3j++ecZPXo0W7ZsYcKECQBcffXVZGdn8/LLL/PII4/Qpk0bAI455pga48jKyuLkk09m/fr1XHXVVfTq1YuVK1fyj3/8g3feeYePP/6Y9PT0Cs+ZOHEi+fn5XH311aSkpPCPf/yDESNG0KVLF0455ZSQxj937lyKi4sZPnw4DRs2ZOjQoTzxxBNkZWXRvHnzCm0vv/xy5s+fT58+fZg0aRItWrRg7dq1vPDCC9xzzz1AMLkeMGAAy5cvJyMjg2HDhpGamsp//vMfXnrpJa6//vqQ4qrOK6+8wqOPPsq1117LNddcQ7NmzYDQ3xeAnTt3cuqpp7JmzRoGDRrEtddeS3FxMStXrmTJkiVccskljB49mkcffZRZs2Zx//33V4hhzpw5FBcXM2rUqD0eR0iccwl1O+6441x9OOX2ZW7M4x/WS98iIrJv+fLLL6vfsWKcc2+evm/dVozbq7H269fPJSUluV9++aXC9hNPPNElJye7LVu2OOec2717tyssLKzy/DvvvNMB7qOPPirbtmHDBge4v/zlLxXaAu6KK64oe7x27VpnZq5fv34uEAiUbf/000+dmTnAbdiwoWx7bm5ulf6Li4vd6aef7po1a1Yhvr/85S9Vnl9q9uzZDnDvvvtu2baJEyc6wD3++OMV2k6bNs0B7s4776zy/B49erjdu3eXbf/pp59ccnKyu+SSS6r0WZMjjjjCnX766WWPV61a5QA3ffr0Cu2ee+45B7hhw4a54uLiKq9BqQcffNABbsKECVX6Kt/uiiuucME0sqrK71Pp+9mwYcNqPxvhvC/XXnutA9yMGTNqje+kk05y7dq1q/B74Zxzhx12mDvyyCOrjbuyGj/H5QArXDX5qEpEImRXQUBrYIuIiO+MHDmS4uJi5s6dW7Zt7dq1fPjhhwwcOLBs9jc5ObmsfjoQCLBjxw62bt1K//79Afjoo4/C7nvRokU457jllltISkoq296rVy/OOuusKu2bNm1adr+goIBt27axfft2MjIyyM7OZu3atWHHUOrll1+mbdu2jBkzpsL2q6++mrZt2/Lyyy9Xec51111HcvLvf/0+6KCD6Nq1a7WlDdX54IMPWLt2LVdccUXZtmOPPZYePXowa9asCm1Ly2YeeuihKrXj5R/Pnz+fli1bVikbqdxuT5x33nkceeSRVbaH+r6UlJTw7LPPcuSRR1Z5nSvHN2bMGDZt2sTSpUvLtr3//vusW7cuYucE1EYlIhGSo1VERESkLsf9rb4jiLiLLrqIFi1aMHv2bG6//XaAsuSucp3r9OnTeeKJJ1izZg0lJSUV9u3YsSPsvr/77jsAjjjiiCr7unXrxhtvvFFhW25uLpMnT2bhwoX8+OOPVZ6zJzGU2rBhA71796Zhw4q5QMOGDenatSufffZZledUV8bSunVrvv/++5D6zMzMpFGjRvTs2ZP169eXbR8wYAAPPvggX3zxRVlZy7p162jXrh37779/rcdct24dPXr0IDU1NaQYwtG1a9dqt4f6vmzdupUdO3Zw9tln19nXkCFDuOmmm8jMzOT8888Hgq9XcnIyw4cP34tRhEYZYQSUlDh2FQRI1yoiIiLiM6mpqVx22WVMnz6dDz74gD59+jBv3jwOPvhgBgwYUNbu4Ycf5tZbbyUjI4Mbb7yRAw88kOTkZH7++WdGjBhRJeGOhssuu4wlS5YwZswY+vbtS+vWrUlKSmLp0qU88sgjMYmhvPKz7uW5EJbZy83NZeHChRQVFdGzZ89q28yaNYu//S06/6mr6QTHQCBQ43OaNGlS7fZovC+NGzdm2LBhzJgxg82bN9O4cWNeeOEFBg4cSNu2bcM+XriUEUbArt3eZdK1ioiIiPjQyJEjmT59OrNnz2b79u38+uuvTJo0qcKf7OfNm0fHjh157bXXKmx//fXX97jf0hngtWvX0rlz5wr7vvzyywqPd+7cyZIlS7j88st54oknKux76623qhw73AvHHXrooXz99dcEAoEKs9iBQIBvvvmm2tnqvbFw4UJyc3O57777OOyww6rsf/TRR3nmmWeYOnUqycnJdO3alUWLFrF58+ZaZ7G7du3K2rVr2b17NykpKTW2a9WqFQDbt28vuw+//1UhVOG8L23atKFly5Z8/vnnIR17zJgxPP7448yZM4fmzZuTl5cXk/IQ0DJ9EZGTXwSgdbBFRMSXevXqRY8ePXjuued4/PHHMbMq5SFJSUmYWYXZ2UAgwAMPPLDH/Q4cOBAz4+GHH6a4uLhs+2effVYlOSudLa48O7xp06Zql4NLS0sDgglkKC688EK2bNlS5VgzZ85ky5Yt/OlPfwrpOKHKzMykVatWjB8/nkGDBlW5jRw5km3btrFo0SIAhg4dCsBtt91WZUa4/GsydOhQduzYwZQpU6r0Wb5dablH5df5r3/9a1jjCOd9adCgAZdeeilffvlltUsRVj7GMcccwwknnMCsWbPIzMykQ4cOZGRkhBXfnlJGGAG5BcEZ7HTNYIuIiE+NHDmSG264gddff50zzjijyoztoEGDmDBhAueccw4XXXQR2dnZLFiwYK8uHHPEEUcwduxYpk2bRr9+/bj44ov57bffmDZtGsceeywrV64sa5uenk5GRgbPPPMMjRs35vjjj+f7779nxowZdOrUqcpa1SeeeCIAt99+O0OHDi1bY7t79+7VxnLbbbfx/PPPM3bsWD777DN69uzJypUryczM5PDDD+e2227b43FWtnbtWj744ANGjBhRpea71MCBA2nUqBGZmZkMHjyYwYMHM2TIEObOncu6desYOHAgLVu25JtvvmHZsmWsXr0aCK4h/eqrrzJlyhQ++eQTMjIySE1NZc2aNXz99ddlCfWll17KxIkTGTNmDGvXrqVVq1a8/vrrbN26NayxhPu+TJkyhXfeeYdRo0bxxhtvcOqpp+KcY+XKlQQCAebNm1eh/ZgxY8qW5PvLX/4Su4sDVbe0SDzf6mOZvk/Xb3MHXPGie2vVppj3LSIi+55QlvdKNNu3b3epqakOcHPnzq2yPxAIuPvuu8917tzZJScnuw4dOrjx48e7L7/8ssqSfKEu0+dccGm2KVOmuA4dOrjk5GR31FFHuWeeeabaZfa2bNniRo4c6dq1a+dSUlJc9+7d3ZNPPlntsnvOBZes69Spk2vYsGGFeGpq/9tvv7lrr73WHXTQQa5hw4buoIMOctddd13ZUoWlanq+c86dfvrp7pBDDqnmFf7dn//8Zwe4xYsX19ouIyPDNWjQwP3www9lr9W0adNcz549XePGjV1aWpo7+uij3eTJkys8Lz8/302ZMsV169bNpaSkuObNm7vevXtXWYLwww8/dCeffLJLSUlxrVu3dqNHj3Y7duyocZm+yu9nqXDflx07drjx48e7zp07u0aNGrlWrVq5U0891T333HNVjp2bm+uaNWvmGjRo4DZu3Fjr61XZ3izTZy6EQvp40rt3b7dixYqY9vnFxh1MmLuK+4b34NiOLWPat4iI7Hu++uqrapcjE5HY2r17N+3ateP4448P+2JGoXyOzexT51zvyttVIhIBx3Rsyf/edWZ9hyEiIiIi5cyfP58dO3ZUu252NCnBFhEREZGE8uqrr/L9998zefJkunXrxoUXXhjT/pVgi4iIiEhCueGGG/jll1847rjjeOqpp2pcczxalGCLiIiISELZuHFjvfavdbBFRERERCJICbaIiIiISAQpwRYREYmCRFsGV8RP9vbzqwRbREQkwho2bEggEKjvMERkDxUVFe3ViZFKsEVERCIsNTWV3Nzc+g5DRPZQdnY26enpe/x8JdgiIiIR1rZtW7Zs2UJeXp5KRUTihHOOwsJCtm7dyo4dO2jVqtUeH0vL9ImIiERYamoq+++/P7/++iu7d++u73BEJERJSUmkp6fToUMHUlJS9vg4SrBFRESioHnz5jRv3ry+wxCReqASERERERGRCFKCLSIiIiISQUqwRUREREQiSAm2iIiIiEgEKcEWEREREYkgJdgiIiIiIhGkBFtEREREJIIs0a4wZWZbgO/rqfs2wFYf9evXvv04Zr/27ccx+7VvP47Zr337ccx+7jvaDnHOta28MeES7PpkZiucc7390q9f+/bjmP3atx/H7Ne+/Thmv/btxzH7ue/6ohIREREREZEIUoItIiIiIhJBSrAj60mf9evXvv04Zr/27ccx+7VvP47Zr337ccx+7rteqAZbRERERCSCNIMtIiIiIhJBSrAjwMzONrOvzWy9md0R4743mtl/zGyVma2Icl+zzOw3M1tdblsrM3vTzNZ5P1vGqN/JZvazN+5VZnZupPv1+mlvZu+a2ZdmtsbMxnnbYzHumvqO6tjNLNXMPjazz71+7/a2dzKzj7zf8+fMLDmS/dbR99NmtqHcmHtEuu9yMSSZ2UozW+I9jvq4a+g3JmOu7jskFr/ftfQdq892CzN7wczWmtlXZnZSDMddXd/R/lwfXu7Yq8ws28xuitF3WU19x+q9vtn7PlltZv/0vmdi8X1WXb+x+lyP8/pdY2Y3edti9ftdXd8xea/3Kc453fbiBiQB3wKHAsnA50C3GPa/EWgTo776Ar2A1eW2TQXu8O7fATwYo34nA3+OwZjbAb28++nAN0C3GI27pr6jOnbAgDTvfiPgI+BEYCFwibf9CeDaGPb9NDAo2u+31+8twAJgifc46uOuod+YjLm675BY/H7X0nesPttzgFHe/WSgRQzHXV3fMRm312cS8CtwSKzGXEPfUR8zcBCwAWjsPV4IjIj257qWfqP+uQa6A6uBJkBD4C2gSyze61r6jtnv975y0wz23jsBWO+c+845Vwg8C1xQzzFFhXPufWB7pc0XEPzHAu/nhTHqNyacc5ucc59593OArwh+ccZi3DX1HVUuKNd72Mi7OaAf8IK3PVpjrqnvmDCzg4HzgKe8x0YMxl25331A1H+/65OZNSf4H/dMAOdcoXNuJzEYdy19x9IfgG+dc98T+/e6fN+x0hBobGYNCSZ+m4jB57qafn+JQh/VORL4yDmX55wLAO8BFxGb97qmvn1HCfbeOwj4sdzjn4hBElSOA94ws0/NbEwM+y21v3Nuk3f/V2D/GPZ9vZl9YcESkqj8qas8M+sI9CQ4qxrTcVfqG6I8dq9cYRXwG/Amwb/S7PS+MCGKv+eV+3bOlY75f7wxP2JmKdHoG/gbcBtQ4j1uTWzGXbnfUrEYc3XfIbH6/a7p+yvan+1OwBZgtgXLcp4ys6bEZtw19Q2x+067BPindz/W3+Hl+4Yoj9k59zPwEPADwcQ6C/iUKH+uq+vXOfeGtzvan+vVwGlm1trMmgDnAu2JzXtdU98Q43+z65sS7Ph3qnOuF3AOMNbM+tZXIC7496FYzTb+A+gM9CD45fXXaHZmZmnAi8BNzrns8vuiPe5q+o762J1zxc65HsDBBP9Kc0Sk+wi1bzPrDkzwYjgeaAXcHul+zeyPwG/OuU8jfew97DfqY/bU+h0S5d/v6vqOxWe7IcGys38453oCuwj+ybxMFMddU98x+U7zao0HAs9X3heD77LKfUd9zF4idwHB/9gcCDQFzo50P6H0a2bDiMHn2jn3FfAg8AbwOrAKKK7UJirvdS19x/Tf7H2BEuy99zO//+8MgknBz7Hq3PtfMs6534CXCSZDsbTZzNoBeD9/i0WnzrnNXiJWAswkiuM2s0YEE9z5zrmXvM0xGXd1fcdy7N6frt8FTgJaeH/qhBj8npfr+2yvXMY553YDs4nOmE8BBprZRoKlXv2AvxP9cVfp18yeidGYa/oOicnvd3V9x+j3+yfgp3J/HXmBYNIbi3FX23cMP9fnAJ855zZ7j2P5HV6h7xiNuT+wwTm3xTlXBLxE8DMX7c91df2eHMPPdaZz7jjnXF9gB8FzeGL1ua7Sdyz/3dpXKMHee58Ah3lnJCcT/PPX4lh0bGZNzSy99D6QQfDPM7G0GLjCu38FsCgWnZZ+SXj+RJTG7dXgZgJfOeceLrcr6uOuqe9oj93M2ppZC+9+Y+AsgvXf7wKDvGbRGnN1fa8t94+CEawbjPj77Zyb4Jw72DnXkeDn+B3n3FCiPO4a+h0WizHX8h0Si9/vavuOxWfbOfcr8KOZHe5t+gPwJTEYd019x+o7DbiUiiUasfwOr9B3jMb8A3CimTXxPkul73W0v8+q67AONK0AAASLSURBVPerWHyuvePv5/3sQLAGegExeq+r6zuGv9/7DrcPnGkZ7zeCNUbfEKxTnRTDfg8luGrJ58CaaPdN8ItxE1BEcBZmJMEa1beBdQTPFm4Vo37nAf8BviD4pdEuSmM+leCf0b4g+KeuVd77HYtx19R3VMcOHAOs9I6/Grir3O/bx8B6gn/iTYnCmGvq+x1vzKuBZ/BWGoni7/oZ/L6aR9THXUO/UR9zTd8hMfr9rqnvWH22ewArvH5eAVrGYty19B31cRMsj9gGNC+3LVZjrq7vWL3XdwNrvc/SPCAlRt9n1fUbk+8y4F8E/yPxOfCHGL/X1fUdk/d6X7rpSo4iIiIiIhGkEhERERERkQhSgi0iIiIiEkFKsEVEREREIkgJtoiIiIhIBCnBFhERERGJICXYIiISEWa23LtgjoiIrynBFhHZh5nZGWbmarkF6jtGERGpqGHdTUREZB/wT2BpNdtLYh2IiIjUTgm2iEh8+Mw590x9ByEiInVTiYiISAIws45eychkM7vUzL4wswIz+8HbVmVCxcyOMbOXzWyb1/ZLM7vNzJKqaXuAmT1qZt+Z2W4z+83M3jSzs6ppe6CZ/dPMdphZnpktM7Ou0Rq7iMi+RjPYIiLxoYmZtalme6FzLrvc44HAocDjwK/e478AhwBXljYys97Ae0BRubbnAw8CxwJDy7XtCPw/YH9gLrACaAqcCPQH3izXf1PgfeBDYCLQCRgHLDKz7s654j0ZvIhIPDHnXH3HICIiNTCzM4B3a2nyv865P3pJ8AaCNdnHO+c+855vwEvAhcBJzrkPve3/D+gD9HLOfVGu7XPAYKC/c+5tb/tS4BzgbOfcskrxNXDOlXj3lwOnA7c756aWazMemFrd80VEEpFKRERE4sOTwFnV3CZVavdmaXIN4IKzKKXJ7p8AzGw/4GRgcWlyXa7t/1Rq2wo4G3i9uuS4NLkupwR4tNK2d7yfh9U5ShGRBKASERGR+LDOOfdWCO2+qmbbl97PQ72fnbyfa2p4fkm5tl0AA1aGGOcvzrmCStu2eT9bh3gMEZG4phlsERGJpNpqrC1mUYiI1CMl2CIiieXIarZ1835+5/3c4P08qpq2RxD8t6G07XrAAT0iFaCISKJTgi0ikljOMrNepQ+8Exdv8x6+AuCc+w34ADjfzLpXajvBe/iy13Y78Bpwjpn1r9yZ9xwRESlHNdgiIvGhl5kNq2HfK+Xufw68Y2aPA5uACwgupTfPOfd/5dqNI7hM37+8tr8CfwQGAAtKVxDxXE8wIX/NzOYAnwKNCa5CshG4fS/HJiKSUJRgi4jEh0u9W3UOAwLe/cXA1wRnog8HfgPu9W5lnHMrzOxk4G7gOoLrV39HMFn+a6W2G7x1s/8bOBcYDuwgmMw/ubcDExFJNFoHW0QkAZRbB/tu59zkeg1GRMTnVIMtIiIiIhJBSrBFRERERCJICbaIiIiISASpBltEREREJII0gy0iIiIiEkFKsEVEREREIkgJtoiIiIhIBCnBFhERERGJICXYIiIiIiIRpARbRERERCSC/n+2fuai1dAlaAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpvBSYrQMKXw"
      },
      "source": [
        "model.save(\"/content/drive/My Drive/tab_audio_classify_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGClOvy-QIK9"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/drive/My Drive/tab_audio_classify_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp67zx64QoHE"
      },
      "source": [
        "## removing the softmax + dropout layers to extract the penultimate latent vector\n",
        "\n",
        "dummy_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "for layer in dummy_model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVnJ7YKERKY-"
      },
      "source": [
        "# every class that is not in the training/val set. \n",
        "non_classes = [c for c in total.speaker_id.unique() if c not in classes]\n",
        "\n",
        "non_class1 = total[total.speaker_id == non_classes[0]].iloc[:,:-1]\n",
        "non_class2 = total[total.speaker_id == non_classes[1]].iloc[:,:-1]\n",
        "non_class3 = total[total.speaker_id == non_classes[2]].iloc[:,:-1]\n",
        "\n",
        "nc1_ex1 = dummy_model(np.expand_dims(non_class1.iloc[0].values, axis=0)).numpy().flatten()\n",
        "nc1_ex2 = dummy_model(np.expand_dims(non_class1.iloc[1].values, axis=0)).numpy().flatten()\n",
        "nc1_ex3 = dummy_model(np.expand_dims(non_class1.iloc[2].values, axis=0)).numpy().flatten()\n",
        "\n",
        "nc2_ex1 = dummy_model(np.expand_dims(non_class2.iloc[0].values, axis=0)).numpy().flatten()\n",
        "nc2_ex2 = dummy_model(np.expand_dims(non_class2.iloc[1].values, axis=0)).numpy().flatten()\n",
        "nc2_ex3 = dummy_model(np.expand_dims(non_class2.iloc[2].values, axis=0)).numpy().flatten()\n",
        "\n",
        "nc3_ex1 = dummy_model(np.expand_dims(non_class3.iloc[0].values, axis=0)).numpy().flatten()\n",
        "nc3_ex2 = dummy_model(np.expand_dims(non_class3.iloc[1].values, axis=0)).numpy().flatten()\n",
        "nc3_ex3 = dummy_model(np.expand_dims(non_class3.iloc[2].values, axis=0)).numpy().flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cCdC9nvSKbv"
      },
      "source": [
        "## for every class in non_classes, find the latent vector representation from our model. Compare this with the representation from every other class\n",
        "count = 0\n",
        "for i in range(len(non_classes)):\n",
        "  test_class = non_classes[i]\n",
        "  test_class_exs = total[total.speaker_id == test_class].iloc[:,:-1]\n",
        "  test_class_ex =  dummy_model(np.expand_dims(test_class_exs.iloc[0].values, axis=0)).numpy().flatten()\n",
        "\n",
        "  smallest_diff = 999999999\n",
        "  best_class = test_class\n",
        "\n",
        "  for classi in non_classes:\n",
        "    class_exs = total[total.speaker_id == classi].iloc[:,:-1]\n",
        "    class_ex =  dummy_model(np.expand_dims(class_exs.iloc[2].values, axis=0)).numpy().flatten()\n",
        "\n",
        "    diff = np.linalg.norm(test_class_ex-class_ex)\n",
        "    if diff < smallest_diff:\n",
        "      smallest_diff = diff\n",
        "      best_class = classi\n",
        "      \n",
        "  if best_class != test_class:\n",
        "    count += 1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBYEW_Abl209",
        "outputId": "f2f0e566-53e1-4789-d6ef-fdab8d05de8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "len(non_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdZwjr2VVzKF",
        "outputId": "f66dabb1-5e7b-4cab-be39-5614aafcc298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "100*(1-count/len(non_classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.3069306930693"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7akSW-gGwhv",
        "outputId": "8ab8f92f-1fda-4c7c-9ce0-f78b9d26ec41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "%%time\n",
        "import tensorflow as tf\n",
        "root = \"/content/reddragon/\"\n",
        "for i in range(len(clip_info)):\n",
        "  row = clip_info.iloc[i]\n",
        "  k = np.load(row['final_path'])\n",
        "  clip_info.loc[i, 'mean'] = k.mean()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n%%time\\nimport tensorflow as tf\\nroot = \"/content/reddragon/\"\\nfor i in range(len(clip_info)):\\n  row = clip_info.iloc[i]\\n  k = np.load(row[\\'final_path\\'])\\n  clip_info.loc[i, \\'mean\\'] = k.mean()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jxZTR4rJtWJ"
      },
      "source": [
        "data_mean = -26.533236022216165"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-RVIzH2OFww"
      },
      "source": [
        "train_labels = pd.get_dummies(train_df.speaker_id)\n",
        "test_labels = pd.get_dummies(test_df.speaker_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhbiUp42rLMK",
        "outputId": "a399b710-5a1e-46fc-9eab-ec7eeab8b9b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "train_labels.shape\n",
        "test_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1337, 112)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8YOHTc3-2Tr"
      },
      "source": [
        "def get_spec_norm(filename, data_mean=-26.533236022216165, spec_width=378):\n",
        "  spec = np.load(filename)\n",
        "  spec /= data_mean\n",
        "\n",
        "  num_times = spec_width // spec.shape[1]\n",
        "  \n",
        "  if num_times != 0:\n",
        "    leftover = spec_width - num_times * spec.shape[1]\n",
        "\n",
        "    spec = np.repeat(spec, num_times, axis=1)\n",
        "    \n",
        "    spec = np.concatenate([spec, spec[:,:leftover]], axis=1)\n",
        "  else:\n",
        "    spec = spec[:,:spec_width]\n",
        "\n",
        "  \n",
        "  spec = np.expand_dims(spec, axis=2)\n",
        "  \n",
        "\n",
        "  return spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aafYwPTrrlSS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}